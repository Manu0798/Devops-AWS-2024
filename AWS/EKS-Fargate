# How to create EKS fargate Cluster manually
-> First we should setup the vpc,efs,ec2 and s3 bucket infrastructure using terraform. once we setup we should execute the below commands

-> You will execute the below command to create EKS fargate cluster within private subnetes after you execute the above tools
-> eksctl create cluster   --name fargate-cluster   --version 1.29   --region ap-south-1   --vpc-private-subnets=subnet-04be94e91560f8286,subnet-06916357d4b0bcaf1   --fargate   --without-nodegroup

# Install AWS Load Balancer Controller
-> eksctl utils associate-iam-oidc-provider   --region ap-south-1   --cluster fargate-cluster   --approve

-> curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json

-> aws iam create-policy   --policy-name AWSLoadBalancerControllerIAMPolicy   --policy-document file://iam_policy.json

-> eksctl create iamserviceaccount   --cluster fargate-cluster   --namespace kube-system   --name aws-load-balancer-controller   --attach-policy-arn arn:aws:iam::<ACCOUNT_ID>:policy/AWSLoadBalancerControllerIAMPolicy   --approve

-> helm repo add eks https://aws.github.io/eks-charts
-> helm repo update

-> helm upgrade -i aws-load-balancer-controller eks/aws-load-balancer-controller   -n kube-system   --set clusterName=fargate-cluster   --set serviceAccount.create=false   --set serviceAccount.name=aws-load-balancer-controller   --set region=ap-south-1   --set vpcId=<your-vpc-id>   --set ingressClass=alb


-> helm list -n kube-system
-> kubectl get pods -n kube-system | grep aws-load-balancer-controller
-> kubectl get pods -n kube-system | grep aws-load      --> Verify AWS Load Balancer Controller is installed

-> kubectl logs -n kube-system deployment/aws-load-balancer-controller
-> kubectl describe pod <pod-name> -n kube-system
-> kubectl logs <pod-name> -n kube-system

#âœ… Tag the public subnet (for the ALB):
Use this if you want the ALB to be publicly accessible (internet-facing):
-> aws ec2 create-tags --resources subnet-036e280494be0ae15 subnet-07f35c51190df9a92   --tags Key=kubernetes.io/role/elb,Value=1          Key=kubernetes.io/cluster/frissly-stag-eks-cluster,Value=owned
# Tag the private subnets (for worker nodes + internal traffic):
-> aws ec2 create-tags --resources subnet-08945387cfad57851 subnet-07ec8a824ee6b07a2   --tags Key=kubernetes.io/role/internal-elb,Value=1          Key=kubernetes.io/cluster/frissly-stag-eks-cluster,Value=owned   
-> aws ec2 describe-subnets   --filters "Name=tag:kubernetes.io/cluster/<your-cluster-name>,Values=owned"   --query "Subnets[*].Tags"
-> aws ec2 describe-subnets --filters "Name=tag:kubernetes.io/cluster/<your-cluster-name>,Values=owned"

-> kubectl get create ns app
-> eksctl create fargateprofile \
  --cluster fargate-cluster \
  --name app-profile \
  --namespace app \
  --region ap-south-1



# To Install the EFS CSI Driver 
-> kubectl apply -k "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/ecr"

# pv.yml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: efs-pv
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteMany  # Allows multiple pods to access the volume
  persistentVolumeReclaimPolicy: Retain
  storageClassName: efs-sc
  csi:
    driver: efs.csi.aws.com
    volumeHandle: fs-0ba7e1f0d3bb627f3  # Replace with your EFS file system ID

pvc.yml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
  storageClassName: efs-sc




# Below command is use to delete cluster
-> eksctl delete cluster --name demo-cluster --region ap-south-1
-> eksctl create cluster   --name demo-cluster   --region ap-south-1   --fargate   --vpc-private-subnets=subnet-023ee67bc1bd60f63,subnet-073004b80be4e8f8f   --without-nodegroup




