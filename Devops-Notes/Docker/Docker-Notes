# What is Docker
Docker is a set of platform as a service product that use os-level virtualization to deliver software in packages is called container
Docker is a platform that allow the application you to package and distribute software application in container
Docker is used to manage the container based application
Deployment into multiple environment easy if we use docker
It is a containerisation software
Docker is a containerization platform that provides easy way to containerize your applications, which means, using Docker you can build container images, run the images to create containers and also push these containers to container regestries such as DockerHub, Quay.io and so on.

# How docker will work 
Docker will abstruct and package developer written code. After that it will create on image. later we will run our machines using that images 

# What is Container
-> container is a way to package application with all the necessary dependencies and configuration
-> Containers having lite weight operating system [Why it is lite weight operating system it doesn't have full operating system. it has base operating system. it get the resources from base OS]
we can run containers in multiple machines easily
container will take every thing which is we required

# Docket Example
You install Docker Desktop on Windows.
Docker internally runs a tiny Linux VM (because Docker requires Linux kernel features like namespaces & cgroups).
Inside Docker → you run containers (Ubuntu container, Red Hat container, MySQL container, Node.js container, etc.).

👉 Difference:
Containers share the same OS kernel (from that tiny Linux VM that Docker Desktop runs).
They don’t need to boot a full OS every time.
Containers are just the application + its dependencies.

# VM Example
🔹 Step 1: You have a Windows Laptop (Host Machine)
This Windows system has CPU, RAM, Disk, and Network resources.

🔹 Step 2: You Install a Hypervisor (VirtualBox / VMware Workstation)
This hypervisor sits on top of Windows (Type 2 hypervisor).
It acts as a resource manager → dividing CPU, RAM, Disk among multiple VMs.

🔹 Step 3: You Create Multiple VMs
For example:
VM1 → Ubuntu Linux (allocated 2GB RAM, 20GB Disk).
VM2 → Red Hat Linux (allocated 3GB RAM, 30GB Disk).
VM3 → Windows Server (allocated 4GB RAM, 40GB Disk).

🔹 How They Work
Each VM behaves like a separate computer.
Each VM has its own full operating system kernel.
But none of them run directly on hardware → they get resources from Windows via the hypervisor.

👉 Example:
If your laptop has 8GB RAM, you can split it like:
Ubuntu VM → 2GB
Red Hat VM → 3GB
Windows Server VM → 3GB
Windows host + hypervisor manages all that distribution.

🔹 Important Point
Yes, each VM has its own OS (full copy, with kernel).
But they rely on the host machine’s physical resources (CPU, RAM, Disk).
That’s why VMs are heavier (every VM duplicates a full OS).

# you will watch below video to understand difference between VM and Sever
-> https://youtu.be/ksDAQX6MbeE?si=FM_VAD9ZLenNs_Fm

# Why Docker instead of VMs?

| Feature         | Virtual Machines                  | Containers (Docker)        |
| --------------- | --------------------------------- | -------------------------- |
| **OS**          | Each VM has its own OS            | Share host OS kernel       |
| **Size**        | GBs (big)                         | MBs (small)                |
| **Startup**     | Minutes                           | Seconds                    |
| **Efficiency**  | Uses more CPU/RAM                 | Uses less, lightweight     |
| **Portability** | Difficult                         | “Build once, run anywhere” |
| **Isolation**   | Strong isolation (via hypervisor) | Process-level isolation    |


# What is the difference between Traditional VMs and AWS Ec2
🔹 Traditional VMs (VirtualBox, VMware, Hyper-V)
Run on top of your own laptop/desktop.
You install a hypervisor (VirtualBox, VMware, Hyper-V).
You create VMs (Ubuntu, Red Hat, Windows Server).
Each VM is allocated CPU/RAM/Disk from your machine’s hardware.
You are the one who has to:
Buy/maintain the physical machine.
Scale up/down manually.
Handle failures (if your laptop dies, all VMs die).

🔹 AWS EC2 (Elastic Compute Cloud)
EC2 is like a VM in the cloud — but managed by AWS.
AWS has giant data centers with racks of servers.
On those servers, AWS runs their own hypervisor (based on Nitro Hypervisor).
   When you launch an EC2 instance:
   AWS hypervisor creates a virtual machine for you on their hardware.
   You don’t see the hypervisor — AWS manages it.
   You only see your Ubuntu / Windows Server instance and use it via SSH/RDP.


# What is Docker Hub
Docker is a cloud based repository where we can share and store images. its like a market place for conatainer


# How to install Docker in redhat
1.sudo yum update -y
2.sudo yum install docker -y
3.sudo service docker start
4.sudo chkconfig docker on
5.sudo usermod -aG docker $USER
6.docker --version
7.sudo service docker start
8.sudo service docker status
9.docker info
If it is not work once you will exit from instance and again you will login

# How to install using ubuntu
-> Install Docker using Below script
# Install Docker
# Add Docker's official GPG key
sudo apt-get update -y
sudo apt-get install ca-certificates curl -y
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

# Add Docker repository
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] \
  https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update -y
sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y
sudo chmod 666 /var/run/docker.sock
sudo usermod -aG docker jenkins
sudo usermod -aG docker ubuntu
sudo systemctl enable docker
sudo systemctl start docker
echo "Docker installation complete."

# Docker Terminolory
1) Docker file 
2) Docker images
3) Docker container
4) Docker Registry
5) Doker engine or Docker demon


# Files and Folders in containers base images
    /bin: contains binary executable files, such as the ls, cp, and ps commands.

    /sbin: contains system binary executable files, such as the init and shutdown commands.

    /etc: contains configuration files for various system services.

    /lib: contains library files that are used by the binary executables.

    /usr: contains user-related files and utilities, such as applications, libraries, and documentation.

    /var: contains variable data, such as log files, spool files, and temporary files.

    /root: is the home directory of the root user.


# Files and Folders that containers use from host operating system
    The host's file system: Docker containers can access the host file system using bind mounts, which allow the container to read and write files in the host file system.

    Networking stack: The host's networking stack is used to provide network connectivity to the container. Docker containers can be connected to the host's network directly or through a virtual network.

    System calls: The host's kernel handles system calls from the container, which is how the container accesses the host's resources, such as CPU, memory, and I/O.

    Namespaces: Docker containers use Linux namespaces to create isolated environments for the container's processes. Namespaces provide isolation for resources such as the file system, process ID, and network.

    Control groups (cgroups): Docker containers use cgroups to limit and control the amount of resources, such as CPU, memory, and I/O, that a container can access.
    

# what docker file
Docker file contains instruction to create an images
Dockerfile is a file where you provide the steps to build our Docker Image.
It contains Domain specific language keywords to build our Docker image

# How to create Docker file [Default file name is Dockerfile]

FROM: To pull the base image

RUN: To execute commands [If you want to execute the linux and bash commands then we will use run]
                          [Run command will excute while creating the image]
CMD: To provide defaults for execute container [CMD instructions will execute while creating the container]
 
ENTRYPOINT: Entrypoint and CMD both are equal We can over write CMD but we cant the Enrtypoint

WORKDIR: To set the working directory

COPY: It copy files from one place to another place

ADD: ADD it is also copy files from one place to another place
But when we use ADD we will copy github data and website URL and It extract the Zip and tar files automatically

EXPOSE: It will Expose your listener Port number

ENV: To set the Env variables

# What is Docker image or container image
-> executable package that includes everything needed to run a piece of software, including the code, runtime, libraries, environment variables, and system tools
Docker image is package that contains everything needed to run a software including the code, runtime, libraries
 and system tools. Its like a snapshot of a Docker containter.

Docker images commands
docker images  --> To see the docker images
docker pull imagename --> To pull the image from docker
docker login  --> Docker hub login in our terminal
docker tag imagename manoj3003/imagename  --> before push you have to tag our image
docker push manoj3003/imagename   --> Push the image to dockerhub
docket build -t imagename .  --> To build the image
docker build -f dockerfilename -t imagename .  --> To build the image file would be in other loaction[pending]
docker rmi imagename  --> To delete the docker image 
docker rmi $(docker images -q)  --> To delete all the images
docker build --no-cache -t myapp:latest .

# What is Docker container
Docker conatainer is runnable instance of docker image

Docker container commands
docker run -d --name nameofcontainer -p 8081:80 imagename --> port mapping  [8081 place is hostip and 80 place is container ip]
docker run containername  --> To run when we execute ctrl+c it will exit
docker create imagename  --> To run the container without exit 
docker start containername --> To start the container
docker stop containername  --> To stop the container
docker ps --> To see the running containers
docker ps -a --> To see the overall containers 
docker exec -it containername /bin/bash --> To enter into the container
you will enter into containter even below below
docker run -it --name con2 img1 /bin/bash --> directly you will enter into the container and container will aslo created
docker start containername
docker attach containername
docker rm containername  --> To delete the container
docker kill containername  --> To delete forcefully
docker rm -f $(docker ps -aq) --> To remove all container at a time
docker stats  --> The docker stats command shows real-time performance metrics (CPU, memory, network, etc.) of running containers

docker ps -s   --> List running containers along with the size of their writable laye
🧪 Example Output:
CONTAINER ID   IMAGE        COMMAND       ...   SIZE
abc123         nginx        "nginx -g…"   ...   1.2MB (virtual 23.5MB)
1.2MB → Changes made since container started (writable layer)
23.5MB → Total size including base image

📌 Writable Layer Size
The writable layer is where all changes made by the container (after it starts) are stored:
Files created/modified/deleted inside the container
Logs written inside the container
Temporary files, cache, etc.
Writable Layer Size = the amount of disk space used by those changes.

The base image size remains the same; the writable layer is unique per container.

docker ps -as --> check even to stopped container also
docker top containerID  --> The docker top command shows you the running processes inside a container

-> sudo dmesg | grep -i oom   --> This command searches your system logs for Out Of Memory (OOM) errors using dmesg

🧪 Example Output:
If your container was killed by OOM, you'd see something like:

Out of memory: Kill process 1234 (node) score 987 or sacrifice child
Killed process 1234 (node) total-vm:123456kB, anon-rss:65432kB
This tells you the kernel killed a process (maybe a Docker container) due to low memory.

📌 Tip: To prevent OOM kills in Docker
Set memory limits when running your containers:

docker run --memory=512m --memory-swap=512m myapp

# what is Docker registry
Docker registry is a place where we can store our docker images

# What is Docker Engine?
-> Docker Engine is the core software platform that enables you to build and run containers.
-> It’s a client-server application, made up of 3 main parts:
-> Docker Daemon (dockerd)
    Runs in the background on your machine.
    Manages Docker objects → images, containers, networks, volumes.
    Listens to Docker API requests.

-> Docker CLI (docker)
    The command-line tool you use (docker build, docker run, docker ps, etc.).
    Sends commands to the Docker Daemon via the API.

-> Docker API
    REST API used by CLI or other programs to talk to the daemon.
    Lets external tools (CI/CD, orchestration systems) integrate with Docker

# What is a Docker Volume?
-> A Docker volume is a storage mechanism used to persist data generated and used by Docker containers.
-> Volumes are managed by Docker (stored under /var/lib/docker/volumes/) and survive container restarts or deletions.

# Advantages of Volumes
-> Persistent storage → Data is not lost when a container stops or is deleted.
-> Shareable → Multiple containers can share the same volume.
-> Decoupled → Data is stored outside the container filesystem, making it portable and safe.
-> Managed by Docker → Unlike bind mounts, Docker volumes are controlled by Docker itself.

# Ways to Create Volumes
-> You mentioned Automation (via Dockerfile) and Manual (via command). Let’s clarify:
-> For real projects, manual named volumes are preferred.
-> Manual (Recommended / Real Usage)
   -> one way
      -> docker volume create mydata
      -> docker run -it --name mycontainer --mount source=mydata,target=/data ubuntu /bin/bash
      -> or (another way)
      -> docker run -it --name mycontainer -v mydata:/data ubuntu /bin/bash

-> Automatic (Anonymous volumes)
   -> If you declare a volume in a Dockerfile:
       FROM ubuntu:latest
      VOLUME ["/data"]
      CMD ["bash"]
  -> When you build & run this image, Docker will automatically create a new anonymous volume mapped to /data

# Useful Docker Volume Commands
-> docker volume ls → list all volumes
-> docker volume inspect mydata → details (path, usage)
-> docker volume rm mydata → delete a volume
-> docker volume prune → delete unused volumes
-> Data is stored under:  /var/lib/docker/volumes/<volume-name>/_data

# Difference Between Bind Mount and Volume
🔹 Bind Mount
-> Maps a host directory directly into a container.
-> Example
      docker run -v /home/user/data:/app/data nginx
-> Whatever is in /home/user/data (on your laptop) appears in /app/data (inside container).
-> Great for development (code syncing).

🔹 Docker Volume
-> Managed by Docker, stored under /var/lib/docker/volumes/.
-> Example:
      docker volume create mydata
      docker run -v mydata:/app/data nginx
-> Safer and better for production since Docker manages lifecycle.

# Transferring Volumes
# Container → Container
-> docker run -it --name newcontainer --volumes-from oldcontainer ubuntu /bin/bash
-> Both containers now share the same volume.
-> Changes in one container are immediately visible in the other.

# Host → Container
-> docker run -it --name mycontainer -v /path/on/host:/path/in/container ubuntu /bin/bash
-> Host directory is directly mounted into container.
-> Useful for testing local files or configs.




# What is Docker Network?
-> A Docker network is a virtual network that allows Docker containers to communicate with each other and with the outside world.
-> It provides isolation (so containers don’t clash with each other) and a way for containers to discover and talk to each other.
-> A container can be attached to multiple networks at the same time.

# Default Docker Networks (when Docker is installed)
-> Bridge (default)
   -> Default driver for containers if you don’t specify a network.
   -> We can access from host with help of IP and port number
   -> Example:
         docker run -d --name myapp -p 8080:80 nginx

-> Host
  -> We can access from host with help of onle IP
  -> Example:
      docker run -d --name myapp --network host nginx

-> None
   -> Completely disables networking.
   -> Useful for security or offline tasks.

# Other Network Drivers
-> Overlay
   -> Used in Docker Swarm / multi-host setups.
   -> Allows containers running on different Docker hosts to communicate securely.

-> Macvlan
   -> Assigns a MAC address to each container so it looks like a real physical device on the network.
   -> Useful if containers need to appear as separate machines in your LAN.

# Common Docker Network Commands
-> docker network ls   --> List all networks
-> docker network inspect <network-name>  --> Inspect a network
-> docker inspect <container-name> | grep -i network    --> Inspect which networks a container is attached to
-> docker network create --driver bridge mynetwork    --> Create a custom network
-> docker run -d --name web1 --network mynetwork nginx   --> Run container on a custom network
-> docker network connect mynetwork web1   --> Connect existing container to another network


Sample project
docker run -d --name con1 --network <newnetworkname> -p 8080:80 imagename
docker run -d --name con2 --network <newnetworkname> -p 9080:80 imagename
After run the above containers and execute the below commands
docker start con1
docker attach con1
apt-get update
apt-get install iputils-ping
ping con2 ip  [Ip avaliable in docker inspect containername]

docker start con2
docker attach con2
apt-get update
apt-get install iputils-ping
ping con1 ip  [Ip avaliable in docker inspect containername]



# What is Docker Compose?
-> Docker Compose is a tool used to define and run multi-container applications.
-> It uses a YAML file (by default docker-compose.yml) where you specify:
      Services (containers)
      Networks
      Volumes
-> With one command, you can start or stop your entire application stack.

# How to install docker-compose
sudo yum update -y
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
docker-compose --version

# Docker Compose Commands
-> docker-compose up → Create and start containers.
-> docker-compose down → Stop and remove containers, networks, volumes (created by compose).
-> docker-compose ps → Show running containers.
-> docker-compose images → Show images used by the services.
-> docker-compose logs → View logs from all services.
-> docker-compose -f <filename> up → Use a custom file instead of docker-compose.yml.



docker log containername

# Docker Restart Policies
When you start a container, you can specify a restart policy with --restart.

🔹 1. no (default)
-> Docker will not restart the container automatically.
-> If it stops/crashes, it stays stopped until you manually run it again.

🔹 2. always
-> Container always restarts if it stops, no matter the exit code.
-> Even if Docker daemon restarts, the container will start again.
⚠️ Important: If you manually stop the container with docker stop, it will stay stopped.
But if the daemon restarts, Docker will start it again.

🔹 3. unless-stopped
-> Same as always, but with one difference:
👉 If you manually stop the container, Docker will not restart it again, even after a daemon restart.
-> Good for services you don’t want auto-starting once you’ve stopped them.

🔹 4. on-failure
-> Container restarts only if it exits with a non-zero exit code (indicating a failure).
-> You can optionally set a retry limit:
   -> docker run --restart on-failure:5 myapp

# Restart always
docker run -d --name test1 --restart always busybox sleep 10  

# Restart unless stopped
docker run -d --name test2 --restart unless-stopped busybox sleep 10  

# Restart on failure (max 3 retries)
docker run -d --name test3 --restart on-failure:3 busybox sh -c "exit 1"









 






