# What is kubernetes
-> k8s is an orchestration container
-> k8s is an opensource software
-> k8s is manage the docker container
-> k8s will take care of container deployment, scalling, descalling and container load balancer
-> k8s is not replacemt of docker
-> k8s is replacement of docker-swarm
-> k8s is developed by google and donted CNCF in 2014 
    [CNCF means cloud native computing foundation]
-> k8s software developed by using Go langauge
-> k8s version relased to market in the year of 2015
-> K8s script write in json format
-> k8s works on clouste mudules

# Why we are using K8S
-> Kubernetes is a powerful open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications.
-> To achive the below features we will use k8s
  -> Auto Scheduling
  -> Self healing capacity
  -> Rollout and Rollback
  -> Auto scalling
  -> Service discovery
  -> Storage orchestration
  -> Secret and configuration management

# What is Docker-swaram
-> Docker Swarm is a container orchestration tool for clustering and scheduling Docker containers.

Note:Docker swarm initialise all the components like apiserver, etcd  in the OS 
But kubernetes initialize all components as a container
So whenever a component goes down then in docker swarm the whole Master node goes down but in k8s if any components goes down a new container recreated with all components

# What is the difference between Docker-swaram and kubernets
 # Docker swarm
	-> Easy to start the cluster
	-> Manual Autoscaling
	-> Need third party tool to monitor like ELK
	-> Auto setup of load balancer
	-> No expernice with production deployment 

  # Kubernetes
	-> Difficult to start the cluster
	-> Automatic autoscaling
	-> Intergrated tool
	-> Manual setup of load balancer
	-> expernice with production deployment 

-> Kubernetes is better suited for large organisations as it offer more scaliability, networking capabilities like policies and huge third party ecosystem support.

# What is the difference between Docker and Kubernetes
  # Docker
   -> Single host
   -> When we are using docker we won't get below features so that you will use kubernets

# What are the features of K8S
-> Auto Scheduling
-> Self healing capacity  [Replication controller]
-> Rollout and Rollback  [Deployment]
-> Auto scalling
-> Service discovery
-> Storage orchestration
-> Secret and configuration management

# What is Auto Scheduling
-> Kubernets will provide the advance schedular option to launch the containers based on your reruirment

# What is Self healing capacity
-> If a containerized app or an application component fails or goes down. Kuberneted re-deploy it to retain the desired state

# Rollout and Rollback
-> If something goes wrong, kubernets will rollback the change for you

# Auto scalling
-> Autoscaling allow you to automatically scale up and scale down your application based on cpu utilization and memory utilization
    Note: Docker-swarm load balancing is manual process
          kubernets supports autoscaling load balancer
# Service Discovery
-> Kubernets will take care the every thing about network.

# Storage orchestration
-> Kubernets will automatically mount the storage

# Secret and configuration management
-> Depoly and update secrets are not exposing in your stack configuration

Kubernets Architecture
1) k8s works on cluster modules
2) In k8s cluster we will have masternode and workernode

# To Communicate with kubernets cluster we have 2 options
-> UI (user interface)
-> Kubectl (CLI S/w)

# What is Kubectl
-> Kubectl is command line interface(CLI) for interacting with Kubernetes clusters
-> Kubectl is used to run the commands on kubernetes cluster

# How many nodes in k8s
-> We have two nodes in k8s 
   -> Master Node  [Master node will manage the worker node in cluster]
   -> Worker node  [Worker will run the tasks assign by master node]

# What are the components in master node
-> API
-> ETCD
-> Scheduler
-> Control manager  [It will detect the state changes means if pod is in dead state it will detect the changes and inform to schedular then schedular will schedule the another pod]

# What is API 
-> API server will act as communicator between develop/Devops and K8s componets

# What is ETCD
-> It act as database in k8s
-> ETCD will stores cluster information such it current state, desitred state, configuration resources and runtime data.
-> Whenever we run k8s application that request goes to the API Server and API server will store the request in ETCD.
-> It store the imformation in key value pair  [key: value,  name: manoj]

# What Schedular
-> It will scheduled the pods for execution which are unschedule in ETCD
-> Schedular will schedule the pods to workernode with the help of Kubelet

# What is Control manager
-> The Controller Manager monitors the current state of your cluster through the API Server and Makes sure the desired state is maintained
we have servel comtrollers in k8s
-> Node Controller
-> Replication Controller
-> Endpoint Controller
-> Deployment Controller

# What are the components in worker node
-> kubelet
-> container runtime [docker]
-> kubeproxy [Network proxy]
-> Pods

# What is kubelet
-> Kubelet will act as a node-agent 
-> kubelet ensure the containers are running healthy in pod 
-> kubelt will interact with container runntime to create the container

# What is container runtime
-> It is responsible for managing the execution and lifecycle of containers within the Kubernetes environment.

# What is kube-proxy
-> kube proxy act as network proxy
-> Kube proxy will maintain network rules on pods
-> The network rules allow the network to communicate your pods from inside and outside of the cluster

# What is Pod
-> Pod is small execution unit in K8s
-> Pod having group of containers
-> Containers will grouped as a pod in order to increase the intelligence of resources
-> Pod will run single container and multi container.


# Kubernets cluster Setup
There are mulitple ways to set up the kubernet cluster
1) Selfmanaged
2) Provider managed

Self managed cluster means we have to setup the k8s cluster to run our application
To create selfmanaged cluster in two ways
1) mini kube (Single node cluster)
2) kubeadm  (multi node cluster)

Provider managed cluster means we will use K8S cluster which is configured by someone
EKS-> Elastic Kubernets services [AWS]
AKS-> Azure Kubernets service [microsoft]
GKE-> Google kubernets service [Google]
IKE-> IBM kubernets service [IBM cloud]

#How to install Kubernetes
Take 3 Ubuntu Instances (20.00)
 1-Master Node (t2.medium instance)
 2-Work Nodes  ((t2.micro instance)

Install below step-1 and step-2 commands in 3 instances
Step-1:- Installing Docker
    1  sudo apt-get update
    2  sudo apt-get install docker.io -y
    3  docker --version
    4  sudo usermod -aG docker $USER
    5  sudo systemctl start docker
    6  sudo systemctl enable docker
    7  sudo systemctl status docker

Step-2:- Installing Kubernetes
   1  sudo apt install curl -y
   2  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add  --> Add GPG Kubernets key
   3  sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"    --> Add the repository
   4  sudo apt-get install kubeadm kubelet kubectl kubernetes-cni -y
   5  sudo apt-mark hold kubeadm kubelet kubectl kubernetes-cni
   6  kubeadm version
   7  sudo swapoff -a       --> disabel the swap
   8  sudo systemctl daemon-reload
   9  sudo systemctl start kubelet
  10  sudo systemctl enable kubelet.service
  11  sudo systemctl status kubelet.service

we will give All traffic security rules in above three instances

Below Step-3 commands execute in only master node
Step-3:- Running and Deploying Kubernetes
    1  sudo kubeadm init
    2  mkdir -p $HOME/.kube
    3  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    4  sudo chown $(id -u):$(id -g) $HOME/.kube/config
    5  kubectl get nodes
    6  kubeadm token create --print-join-command
Copy that token and past that token in worknodes with sudo  --> worknodes connect to masternode
    7  kubectl get nodes

When you get status is Notready then we will below command
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

# Create the one ubuntu instance to install kubectl (20.00)
Step-4:- Install kubectl in below instance 
1 sudo apt-get update
2 curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
3 chmod +x kubectl
4 sudo mv kubectl /usr/local/bin/
5 kubectl version --client

# How to connect Kubectl to the master node
1 mkdir ~/.kube         --> execute in kubectl
 Note: Take kube config file from master and it keep it here
2 cat ~/.kube/config    --> executive in master node and copy the that data
3 vi ~/.kube/config     --> past here that copy data
4 kubectl cluster-info  --> execute this command in kubectl  --> it showing wheather kubectl is connected to the master node or not
5 kubectl get nodes     --> execute in both kubectl and master to get pods are connected or not
6 sudo systemctl restart kubelet.service   --> if pods are not showing then you will execute this command in master

We have to follow below steps in worknodes and to get pods even in work nodes
1 mkdir ~/.kube         --> execute in worknode
 Note: Take kube config file from master and it keep it here
2 cat ~/.kube/config    --> executive in master node and copy the that data
3 vi ~/.kube/config     --> past here that copy data in worknode
4 kubectl get nodes     --> execute in both kubectl and master to get pods are connected or not

# Kubernetes Core Components
1) Containter
2) Pod
3) Namespaces
4) Service
5) Deployment
6) Replication Controller
7) Replicationn set
8) Daemon set
9) persistent volumes
10) stateful sets
11) Role

Running Notes
-> we are using Docker to create container for application
-> Docker will be used as runtime engine in k8s cluster
-> Kubernets is used to manange our Docker container
-> Kubernets will manage our container but not directly. It will use pod to manage the container
-> Pod is smallest execution unit which we can deploy in k8s cluster
-> In docker container is process
-> In kubernets pod is process

Note: In Docker container is smallest part where we can deploy
      In Kubernets pod is smallest part where we can deploy

# What is Namespace
-> In Kubernetes, a namespace is a virtual cluster within a Kubernetes cluster. 
-> It is used to organize and scope resources within the cluster, providing a way to divide cluster resources between multiple users, teams, or projects. 
-> Namespaces are primarily used to create isolated environments and prevent resource conflicts between different applications or users.
-> kubernets components will be grouped logically using namespaces
Note: we can consider namespace as a java package
-> we will get 3 namespaces when we setup the kubernets cluster
1) default -> It doesnt specify any namespace
2) Kube-system -> It contains K8S Control panal Components
3) Kube-public  -> It is reserved for k8S system usage

Note: It is not recommended to deploy our pods using default namespaces. we have to create our own namspace
      we will run our application using custom namespace
Note: If you delete the namespace all components or resource or objects also deleted

# How to create the namespaces
-> kubectl create namespace <namespacename> --> To create the namespace
-> kubectl delete namespace <namespacename> --> To delete the namespace
-> Kubectl get namespaces or kubectl get ns  --> To see the namespaces 
-> kubectl get pods -n <namespacename>  --> To get pods on particular namespace
-> kubectl get pods --all-namespaces  --> To get all pods on all namespaces
-> kubectl get pods -n kube-system  --> To get Control panal Components 
-> kubectl get pods --> To get pods by default namespace
-> kubectl delete all --all -> To delete all pods,svc,namespaces

# What is Pods
-> Pod is small execution unit in K8s
-> Pod having group of containers
-> Containers will grouped as a pod in order to increase the intelligence of resources
-> Pod will run single container and multi container
-> Pod will execute the nodes, one node will execute the multiple pods


Note:Whenever we go give a command like kubectl run then fst reqst goes to API server 
Api server --> talk to all components of k8s talk to Scheduler to schedule a pod
scheduler sees which worker node has resources to schedule that pod and give that info to API server 
Api server again talk to kubelet to create a pod
Kubelet cretes pod and ask container runtime to create container after everything done kubeproxy take care of ipadress

Kubelet gives all this info to API server,
Api server stores this info in etcd

# Pod Lifecycle
-> We will create the pod using manifest file(yml) 
-> After execute the manifest file that request will go to API server
-> API server validate the request and store that pod request in ETCD
-> Scheduler will find the unschedule pods in ETCD and schedule that pods to a suitable Node according to the resources
-> Once the Scheduler assigns a Pod to a specific Node, that Pod's specification (its details) is stored in the Kubernetes API Server with the assigned Node name.
-> kubelet
     Pulls the Pod spec from the API Server.
     Interacts with the container runtime to start the container(s).
     Monitors Pod and Node health.

-> Container runtime
    Pulls the required container image from a container registry.
    Creates and starts the container(s) within a Pod.

-> Kubeproxy
   Maintains network connectivity and load balancing.
   Ensures the Pod can send/receive traffic through Kubernetes Services.

-> Controller Manager Ensures the system moves towards the desired state. Detects if additional actions are needed (e.g., create ReplicaSets, replace failed Pods).

after the containers are successfully started, the Kubelet sends a status update back to the API Server, and that’s how you see the final result when you check your Pod’s status.
he Kubelet continuously reports back to the API Server with the Pod’s current status (e.g., Pending, Running, CrashLoopBackOff, etc.).


1. kubectl apply -f pod.yaml 
2. --> API Server validates and stores in etcd
3. --> Scheduler assigns Pod to a Node
4. --> Controller Manager ensures desired state
5. --> Kubelet on Node picks up the Pod
6. --> Kubelet triggers Container Runtime
7. --> Containers are created container in pod
8. --> Pod is running
9. --> Kube Proxy enables network access for pod
10. --> After the containers are successfully started in pod, the Kubelet sends a status update back to the API Server, and that’s how you see the final result when you check your Pod’s status.


# How to create the pod
we can create the pod in two ways
1) Interactive Approach
2) Declarative Approach

Interactive Approach
Interactive Approach means we will create pod using command
kubectl run --name conatinername --images=ashokit/javawebapplication

Declarative Approach
Declarative Approach means we will create the pod using manifest file(yml)
apiVersion -> It represents the version
kind -> What is the purpose of the manifeast file 
metadata -> It represents the lables
spec -> What do you want to use this manifeast file
filename default is .yml

create pod using pod manifest file with below commands

kubectl get pods --> To see the pods
Kubectl apply -f <podfilename> --> To execute the pod manifestfile
kubectl delete pod <podname> --> To delete the pod
kubectl describe pod <podname> --> To describe the pod 
kubectl get pods -o wide -> To see on which node pod is running

Note: Pod can access only within cluster by default. Pod cant access the outside of the cluster
curl podip:portnumber -> To access the pod inside the cluster



Note: Pod is ephemeral [It lives short period time]
      when pod is recreated pod will change
      It is not recommand to access the pod using pod Ip

-> Thats why we will use kubernets service components to exexute the pods
-> Kubernetes service will make POD accessible/disaccessible inside and outside of the cluster
-> when we create the service we will get one virtual Ip
-> Cluster Ip will be registered in K8S DNS

# What is K8S Service
-> Kubernetes service will make our POD accessible/disaccessible inside and outside of the cluster
we have three types of Services
1) Cluster IP 
2) Nodeport
3) Load Balancer
4) ExternalName Service
5) Headless Service

# Cluster Ip
-> If we use cluster ip type in service manifest file. We can access only within cluster

# Nodeport Ip
-> If we use Nodeport type in service manifest file. we can access the pod inside and outside of the cluster with static port
-> We can access our service using workernode public Ip and port number
-> When we execute the service manifest file using Nodeport then we will get one random port number
-> [Port number range in K8S is 30000 to 32767]

# Load Balancer
-> If we use LoadBalancer type in service manifest file. we can access inside and outside of the cluster by using 
 loadbalancer DNS URL

# What is the difference between NodePort and Load Balancer
NodePort exposes a service on a static port on each node's IP, allowing external access to the service, while LoadBalancer automatically provisions an external load balancer to distribute traffic across multiple nodes or pods, providing a more robust and scalable solution for external access.

# ExternalName Service
An ExternalName service allows a Kubernetes service to act as an alias to an external DNS name (e.g., an external database, API, or third-party service).
it returns a CNAME record pointing to the external host.

apiVersion: v1
kind: Service
metadata:
  name: external-mysql
spec:
  type: ExternalName
  externalName: mydb.production.example.com  # 👈 External DNS


# Headless Service
->  Headless service (clusterIP: None) is needed so each pod gets a stable DNS name, 
    e.g., mongo-0.mongo.default.svc.cluster.local
-> It is mostly used for stateful application, where the clients need to connect directly to specific pods.

apiVersion: v1
kind: Service
metadata:
  name: mongo
spec:
  clusterIP: None  # headless
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017




# What are the difference between pod manifest file and service manifest file
Pod manifest file having lables
service manifest file having selectors

Create pod with service manifestfile using below commands
kubectl get svc --> To get services
kubectl apply -f <servicefilename> --> To execute the pod manifestfile
kubectl delete service <servicefilename> --> To delete the service filename
kubectl describe service <servicefilename> --> To describe the service name

After execution the servermanifest file you will paste your instance public ip and portnumber in browser
curl serviceip:portnumber  --> To access the pod inside the cluster

-> In above scenario we have create the pod manually[Its not recommand]
-> when we create the pod k8s will not provide high avaliablity
Note: Once pod got delete k8s will not create the another pod and application went down we cant access
-> If we want to get high avaliablity we wont create pod manually
-> we need to use replication controller and replication set to create pod then we will get high avaliablity

Note:High avaliablity means always our application should be accessable

# What is replication controller [Self Healing capabilities]
-> It is one of the key feature in k8s
-> It is responsible to manage POD lifecycle
-> Replication controller is providing to create the multiple pods
-> we can scale up and scale down the pods using replication controller
-> Using Replication controller we can achive high avaliablity
-> Replication controller and pods are associated with pods and selectors

Note: If any pod got down or die it will create the another pods

we will create pod with replication controller manifestfile using below commands
kubectl get svc --> To get services
kubectl get pods --> To get the pods
kubectl apply -f <replicationfilename> --> To execute the pod manifestfile
kubectl scale <resource type> <deployment name> --replicas=<number-of-replicas>
      resource type means kind
      deployment name metadata name

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 1

kubectl scale deployment my-deployment --replicas=3 --> To scale up the replicas
kubectl scale deployment my-deployment --replicas=0 --> To scale down the replicas

# what is ReplicationSet
-> It is next generation of Replication controller
-> It is used to managed the pod life cycle
-> we can scale up and scale down the pods using replication set
-> The only difference between replication controller and replication set
  we have two types selectors
 1) Equality Selector
 2) Set-based Selector

# Equality Selector
-> If we use Equlity Selector you can give only one selector
-> Replication controller will supports the Equality Selector

# Set-based Selector
-> If we use Set-based Selector you can give only multiple selector
-> Replication set will supports the Set-based Selector


# what is deployment 
-> By using deployment we can rollout and rollback our application deployment
-> we can achive the auto-scalling by deployment
-> Deployment is used to tell kubernets how to create and or modify the instaces of pods

# What are Deployment Strategies in Kubernets
-> Recreate deployment strategy
-> Rolling deployment strategy
-> Blue and green apporach [It is not a strategy it is a approach]
-> Canary Deployment
-> A/B Testing
-> Shadow Deployment
-> Staggered Deployment
-> Rollback Deployment


1. Recreate Strategy
Process: Stops all old pods first, then starts all new pods.
Pros: Simple to implement, ensures a clean slate.
Cons: Causes downtime because no pods are available during the switch.

2. Rolling Deployment Strategy  (Default in Kubernetes.)
Process: Replaces pods gradually — old pods are terminated one by one while new ones are added.
Pros: No downtime, safer than recreate. Easy rollback by restarting old image version.
Cons: Temporary mix of old and new versions. Risk of incompatibility if versions can’t work together.

3. Blue-Green Approach (an approach, not a built-in strategy)
Process: Keep two complete environments — Blue (current) and Green (new).
Traffic is switched from Blue to Green once Green is ready.
Pros: Instant rollback by switching traffic back.
Cons: Requires double infrastructure temporarily, higher resource cost.

4. Canary Deployment
Process: Release new pods to a small percentage of traffic first, then gradually increase if no issues occur.
        Release the new version to a small subset of users before rolling it out to everyone. 
Pros: Early detection of problems. Allows safe testing in production with real traffic.
Cons: More complex routing and monitoring needed.

| Strategy/Approach | Downtime | Resource Usage During Deploy | Traffic Mix   | Rollback Complexity |
| ----------------- | -------- | ---------------------------- | ------------- | ------------------- |
| **Recreate**      | High     | Low                          | No            | Medium              |
| **RollingUpdate** | None\*   | Slightly higher              | Yes           | Low                 |
| **Blue-Green**    | None     | High                         | No            | Very Low            |
| **Canary**        | None     | Medium                       | Yes (small %) | Low–Medium          |




Advantages of Blue and green approach
1) Rapid releasing
2) Simple rollbacks
3) Built-in-disaster-recovery
4) Seamless cutomer experience
5) Zero Down time

Running Notes
-> Live code is running in Blue env 
-> First we need deploy in green env after code tested in green env then traffic will divert into blue env

Note: what are the difference Prod Env and Preprod or Nonpod Env
Prod Env means nothing but a live code
None Prod ENV are used for application testing like Dev,Sit,UAT env
-> Dev:- Devlopers will use Dev env for integration testing
-> SIT will use by software testing team to test our application
-> UAT will use by client to test the code
-> Pilot env is called pre-prod environment
-> Prod env will be available for public access or live code

# How to do Blue and Green Deployment
-> First you will deploy the bluedeployment.yml [old code] [live code]
-> After that you will deploy the blueservice.yml
-> later on you will access the application in browser with instance public Ip and service portnumber
-> later on you will deploy the greendeployment.yml [you will test the code before deploy the live code]
-> later on you will deploy the greenservice.yml
-> later on you will access the application in browser with instance public Ip and service portnumber
-> Green env is woking good after the deployment. 
-> lateron you go to the blueservice.yml file and change the file selecters with greenservice.yml file selectors
-> After change you will access the browser blueservice.yml file with port number.

-> kubectl get deployment  --> To see the deployments
-> kubectl delete deployment <deploymentname>  --> To delete the deployment

Note: When we delete the deployment pod will delete the automatically but service wouldnt delete

# What is Canary Deployment
-> Introduce a new version of the application to a subset of users or traffic gradually. 
-> Monitor for issues and progressively increase the rollout if everything goes well, or roll back the changes if issues are detected.



# What is the difference between Replica and Autoscaling
-> Replica it will create another pod when pod will delete.
-> Autoscaling it will create the another pod when load increase on pod
-> When we use autoscaling in EKS. It will create workernode when worknode will die 

# What is HELM 
-> Helm is a package manager for K8S application
-> Helm allows you to install or deploy application on K8S cluster in a smillar mannaer to yum/apt for linux system
-> Helm manages K8S resources package through charts
-> A chart is collection of files organized in specific directory structure

# To install helm
sudo snap install helm --classic

# How to install the HELM
-> wget https://get.helm.sh/helm-v3.7.1-linux-amd64.tar.gz
-> tar -zxvf helm-v3.7.1-linux-amd64.tar.gz
-> sudo mv linux-amd64/helm /usr/local/bin/helm
-> helm version
-> helm list

# What is Prometheus [ It is a monitoring tool]
-> Prometheus is an open-source system monitoring and alerting toolkit.
-> Prometheus collect and store its metrics as time series data
-> Prometheus out-of-the box monitoring capabilities for the K8S container orchestration platform
 
Note:Prometheus will collect the data and Prometheus will give the data to grafana and grafana analysis the data and
   generate the charts and graphs for users

# What is Grafana [It is a visulation tool]
-> Grafana is database analysis and monitoring tool
-> Grafana is multiplatform open source analytics and interactive visualiization web application
-> Grafana will connect with prometheus with data source 

# How to install Prometheus and Grafana
-> helm repo add stable https://charts.helm.sh/stable
-> helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
-> helm repo update
-> helm search repo prometheus-community
-> helm install stable prometheus-community/kube-prometheus-stack    [Due to stack we will install both at a time]
-> helm list
-> kubectl get nodes
-> kubectl get svc

When we installed the Prometheus and Grafana then defaulty both will run on ClusterIP Service
If we want to Monitor we will change the Services and give LoadBalancer
-> kubectl edit svc stable-kube-prometheus-sta-prometheus  --> go to the command and edit the service in prometheus
-> kubectl edit svc stable-grafana  --> go to the command and edit the service in grafana
-> kubectl get svc
-> To access Prometheus in web interface and copy Loadbalancer URL and port number 9090
-> To access Grafana in web interface and copy Loadbalancer URL and port number 80
-> Grafana Default username and passwd
  username: admin
  password: prom-operator

# What is ELK Stack [To monitor the application logs]
-> The ELK stack is a collection of three open-source
   E: Elastic Search -> For store the logs
   L: Log stash -> Enchances the data and sent to the Elastic search
   K: Kibana -> It display the data stored in elastic search [Kibana is a monitoring tool for logs]
-> It allows you search the all logs in single place

We can get different data from application
-> File Beat -> Log files
    It will collect the logs from application and defaulty data will send automatically to elastic search
-> Metric Beat -> To get metrics from application
-> Packet Beat -> To get networkdata from application
-> Heart Beat -> To get Uptime monitoring

Running Notes
What should devops engineer to do
-> Devops Engineer is only setup ELK stach in their cluster 
-> Developer will moniter the logs by uning kibana

# How to setup ELK stash
-> First we need to install Helm in your cluster before
-> Nodes must be in 4Gb RAm
-> Machines with kubectl and helm configured
-> kubectl create ns efk  --> Create namespace on efk
-> kubectl get ns  --> To see the namespaces
-> helm repo add elastic https://helm.elastic.co  --> To add the helm repo
-> helm repo ls  --> To see repository list
-> helm install elasticsearch elastic/elasticsearch -f elasticsearch.values -n efk --> To install the Elastic search
  If you want to do any changes before install in elasticsearch. you will follow below steps
  -> helm show values elastic/elasticsearch >> elasticsearch.values
  -> vi elasticsearch.values
     do changes replicas and masternodes if you need
-> helm ls -n efk
-> kubectl get all -n efk
-> After complete the above the stpes you have to install kibana
-> IF you want to do any changes in kibana before install we need to follow the below steps
   -> helm show values elastic/kibana >> kibana.values
   -> vi kibana.values
  Do changes replicas, service type you will change from clusterIP to LoadBalancer, port number 80
-> helm install kibana elastic/kibana -f kibana.values -n efk  --> To install the kibana
-> After install the kibana you should install logstash or file beat
-> helm install file beat elastic/kibana -n efk  --> To install the file beat
-> When we install the file beat it install as a daemon set
   -> Daemon set is a copy of the pod should be execute in all the worknodes
-> kubectl get all -n efk  --> To see the all data on ns

suppose if you want to install metric beat to see the metric of application then you will follow the below commands
-> helm install metricbeat elastic/metricbeat -n efk  --> To install metric beat
-> If you want to see the logs in terminal you will use the below commands
-> kubelet logs pod <podname>  --> To see logs on pod name


# Kubernetes Workload Types

| Workload Type   | Description                                                                | Use Case Example                          |
| --------------- | -------------------------------------------------------------------------- | ----------------------------------------- |
| **Pod**         | The basic unit. A group of one or more containers.                         | Not used alone in production (no restart) |
| **Deployment**  | Manages **stateless** applications. Ensures the desired number of pods.    | Web servers, APIs                         |
| **ReplicaSet**  | Ensures a specific number of pod replicas. Used internally by Deployments. | Rarely used directly                      |
| **StatefulSet** | Manages **stateful** applications. Gives pods stable identity & storage.   | Databases, Kafka, Zookeeper               |
| **DaemonSet**   | Ensures a pod runs **on all (or selected) nodes**.                         | Log agents, monitoring agents             |
| **Job**         | Creates pods that run **once until success**, then stop.                   | Data processing, backups, one-time tasks  |
| **CronJob**     | Runs jobs on a **schedule** (like Linux `cron`).                           | Scheduled reports, periodic cleanups      |
