# Install grafana 
-> https://grafana.com/grafana/download
-> sudo apt-get install -y adduser libfontconfig1 musl
-> wget https://dl.grafana.com/enterprise/release/grafana-enterprise_11.5.2_amd64.deb
-> sudo dpkg -i grafana-enterprise_11.5.2_amd64.deb
-> sudo /bin/systemctl daemon-reload
-> sudo /bin/systemctl enable grafana-server
-> sudo /bin/systemctl start grafana-server
-> sudo /bin/systemctl status grafana-server


# To get email notification we need to configure below thinsgs in this file sudo vi /etc/grafana/grafana.ini

[smtp]
enabled = true
host = smtp.gmail.com:587
user = varmapotthuri4@gmail.com
# If the password contains # or ; you have to wrap it with triple quotes. Ex """#password;"""
password = dzxbhazwoggfybxx
;cert_file =
;key_file =
skip_verify = true
from_address = varmapotthuri4@gmail.com
from_name = Grafana

-> sudo /bin/systemctl daemon-reload
-> sudo /bin/systemctl restart grafana-server
-> sudo /bin/systemctl status grafana-server

-> After that copy the ip paste it in web browser along with port number 3000 
-> Add the promethous as a data source in grafana
-> Set the alert, Click the alerting, contact point, click create contact point, give name, choose Email, give mail id, click test for sample
-> click save contact point
-> Click on notification policy, click more on defult, click edit, choose above create contact poin, click update policy
-> click alert rule, 



âœ… Summary of Monitored Metrics for node exporter
# Category	Metrics Monitored
-> CPU	Usage, Load Average, Idle Time
-> Memory	Used, Free, Available Memory
-> Disk	Usage, Read/Write IOPS
-> Network	Bandwidth, Errors, Packets Dropped
-> System	Uptime, Running Processes, Blocked Processes

-> Queries related CPU
100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
node_load1
node_load5
node_load15
100 - (rate(node_cpu_seconds_total{mode="idle"}[5m]) * 100)

-> Queries related memory
node_memory_MemTotal_bytes
node_memory_MemFree_bytes
100 * (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes))
100 - ((node_filesystem_avail_bytes{mountpoint="/"} * 100) / node_filesystem_size_bytes{mountpoint="/"})


-> Queries related Disk
rate(node_disk_reads_completed_total[5m])
rate(node_disk_writes_completed_total[5m])
rate(node_disk_read_bytes_total[5m])
rate(node_disk_written_bytes_total[5m])

-> Queries related Network
rate(node_network_receive_bytes_total{device="eth0"}[5m])
rate(node_network_transmit_bytes_total{device="eth0"}[5m])
rate(node_network_receive_errs_total[5m])
rate(node_network_transmit_errs_total[5m])
rate(node_network_receive_drop_total[5m])
rate(node_network_transmit_drop_total[5m])

-> Queries related System health
node_time_seconds - node_boot_time_seconds
node_procs_running
node_procs_blocked




# For Blackbox exporter
1ï¸âƒ£ HTTP/HTTPS Endpoint Monitoring
Availability & Response Time (probe_success, probe_duration_seconds)
HTTP Status Codes (probe_http_status_code)
SSL Certificate Expiry & Validity (probe_ssl_earliest_cert_expiry)
Redirects & DNS Resolution Time (probe_http_redirects, probe_http_duration_seconds)

probe_success{module="http_2xx"}
(probe_ssl_earliest_cert_expiry - time()) / 86400
avg(probe_icmp_duration_seconds{phase="rtt"})
probe_success{module="tcp_connect"}
avg(probe_dns_lookup_time_seconds)


ðŸ“Œ How to Monitor These Application Metrics?
Metric	What It Means?	How to Monitor?
Latency	Response time of requests	Prometheus (Application metrics)
Traffic	Request rate (RPS), data transfer	Prometheus (HTTP request counters)
Errors	HTTP 4xx, 5xx errors	Prometheus + Loki (logs)
Saturation	Resource limits (CPU, memory, I/O)	Node Exporter + Prometheus
Logs	Application logs, errors, warnings	Loki + Promtail
Traces	End-to-end request tracking	Grafana Tempo (Tracing)


histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
sum(rate(http_requests_total[5m]))
rate(node_network_receive_bytes_total[5m])
sum(rate(http_requests_total{status=~"5.."}[5m]))
{job="my-app"} |= "ERROR"
100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100

# loki
{job="my-app"} |= "WARN"
{job="nginx"} |= "500"


# Import Dash board numbers 
-> 1860   --> Node exporter
-> 7587   --> Balckbox exporter
-> 13041  --> Website
-> 13659  


1. Instance Down
Query:

promql
Copy
Edit
up == 0
Description: Alerts if an instance is down.

2. Website Down (Blackbox Exporter)
Query:

promql
Copy
Edit
probe_success == 0
Description: Alerts if the website is not responding to the probe request.

3. High Network Traffic
Query:

promql
Copy
Edit
rate(node_network_receive_bytes_total[5m]) > 100000000 OR rate(node_network_transmit_bytes_total[5m]) > 100000000
Description: Triggers an alert if network traffic exceeds 100MB in 5 minutes.

4. Host Out of Memory
Query:

promql
Copy
Edit
(node_memory_MemAvailable / node_memory_MemTotal * 100) < 25
Description: Alerts if available memory falls below 25%.

5. Host Out of Disk Space
Query:

promql
Copy
Edit
(node_filesystem_avail{mountpoint="/"} * 100) / node_filesystem_size{mountpoint="/"} < 50
Description: Triggers when root (/) disk has less than 50% available space.

6. High CPU Load
Query:

promql
Copy
Edit
(1 - avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m]))) * 100 > 80
Description: Alerts if CPU utilization exceeds 80%.

7. Service Unavailable
Query:

promql
Copy
Edit
up{job="node_exporter"} == 0
Description: Triggers when Node Exporter service is unavailable.

8. High Memory Usage
Query:

promql
Copy
Edit
(node_memory_Active / node_memory_MemTotal) * 100 > 90
Description: Alerts when memory usage exceeds 90%.

9. File System Almost Full
Query:

promql
Copy
Edit
(node_filesystem_avail / node_filesystem_size) * 100 < 10
Description: Alerts if free disk space is below 10%.

10. Docker Container Restarting Frequently
Query:

promql
Copy
Edit
increase(docker_container_restart_count[5m]) > 3
Description: Triggers when a Docker container restarts more than 3 times in 5 minutes.

11. Docker Container High CPU Usage
Query:

promql
Copy
Edit
(rate(docker_container_cpu_usage_seconds_total[5m]) * 100) > 80
Description: Alerts if Docker container CPU usage exceeds 80%.

12. Docker Container High Memory Usage
Query:

promql
Copy
Edit
(docker_container_mem_usage_bytes / docker_container_mem_limit_bytes) * 100 > 90
Description: Triggers when a container uses more than 90% of allocated memory.

13. Docker Disk Usage High
Query:

promql
Copy
Edit
(docker_data_dir_disk_used_bytes / docker_data_dir_disk_total_bytes) * 100 > 85
Description: Alerts if Docker disk usage exceeds 85%.

14. Docker Container Stopped
Query:

promql
Copy
Edit
count by (instance, name) (docker_container_last_seen{state!="running"}) > 0
Description: Triggers when a Docker container stops running.

15. SSL Certificate Expiring
Query:

promql
Copy
Edit
ssl_cert_expires_in < 30 * 24 * 60 * 60
Description: Alerts if an SSL certificate is expiring within 30 days.

16. SSL Certificate Expired
Query:

promql
Copy
Edit
ssl_cert_expires_in <= 0
Description: Triggers when an SSL certificate has expired.

17. High Application Latency
Query:

promql
Copy
Edit
histogram_quantile(0.99, sum by (instance, job) (rate(application_latency_seconds_bucket[5m]))) > 0.5
Description: Alerts if 99th percentile application latency is above 500ms.

18. Critical Application Latency
Query:

promql
Copy
Edit
histogram_quantile(0.99, sum by (instance, job) (rate(application_latency_seconds_bucket[5m]))) > 1
Description: Triggers when 99th percentile latency exceeds 1 second.

19. Saturation Alert
Query:

promql
Copy
Edit
(avg by (instance) (node_load1) / count by (instance) (node_cpu_seconds_total{mode="idle"})) * 100 > 80
Description:

Saturation refers to system resource exhaustion, typically CPU load compared to available cores.
This alert triggers if CPU load is greater than 80% of available cores.


1. Errors (HTTP 4xx, 5xx) - Prometheus + Loki
Query: Count of 4xx and 5xx Errors
logql
Copy
Edit
sum(count_over_time({job="cloudwatch_logs"} |= "HTTP" |~ "4[0-9][0-9]|5[0-9][0-9]" [$__interval]))
ðŸ‘‰ Explanation:

{job="cloudwatch_logs"}: Targets CloudWatch logs being scraped by Promtail.
|= "HTTP": Filters logs containing "HTTP" (common in access logs).
|~ "4[0-9][0-9]|5[0-9][0-9]": Matches HTTP status codes in the 400 and 500 ranges.
count_over_time(... [$__interval]): Counts occurrences in the specified Grafana dashboard interval.
Query: Rate of 4xx and 5xx Errors
logql
Copy
Edit
rate({job="cloudwatch_logs"} |= "HTTP" |~ "4[0-9][0-9]|5[0-9][0-9]" [5m])
ðŸ‘‰ Shows how frequently errors occur over a 5-minute window.

Query: Percentage of 4xx and 5xx Errors
logql
Copy
Edit
sum(rate({job="cloudwatch_logs"} |~ "HTTP/[0-9.]+\" (4[0-9][0-9]|5[0-9][0-9])" [5m]))
/
sum(rate({job="cloudwatch_logs"} |~ "HTTP/[0-9.]+" [5m]))
ðŸ‘‰ Calculates error rate as a percentage of total HTTP requests.

2. Application Logs, Errors, and Warnings - Loki + Promtail
Query: All Logs from the Application
logql
Copy
Edit
{job="varlogs"}
ðŸ‘‰ Shows all application logs.

Query: Filter Logs for Errors
logql
Copy
Edit
{job="varlogs"} |= "error"
ðŸ‘‰ Filters logs containing the word "error".

Query: Count of Errors Over Time
logql
Copy
Edit
count_over_time({job="varlogs"} |= "error" [$__interval])
ðŸ‘‰ Counts occurrences of "error" in logs.

Query: Warnings in Logs
logql
Copy
Edit
{job="varlogs"} |= "warning"
ðŸ‘‰ Filters logs for "warning" messages.

Query: Count of Warnings Over Time
logql
Copy
Edit
count_over_time({job="varlogs"} |= "warning" [$__interval])
ðŸ‘‰ Counts occurrences of "warning" messages.

Bonus: Combine Errors and Warnings
logql
Copy
Edit
{job="varlogs"} |= "error" or {job="varlogs"} |= "warning"
ðŸ‘‰ Fetches both errors and warnings from application logs.




1ï¸âƒ£ HTTP5xxErrors (Critical Severity)
Purpose:
Detects a high number of HTTP 5xx errors (server errors) in logs.
The alert fires when there are more than 5 HTTP 5xx errors in the last 5 minutes.
Expression Breakdown:
yaml
Copy
Edit
sum(rate({job="loki"} |~ "HTTP/1.1" |~ "5\\d{2}" [5m])) by (instance) > 5
job="loki" â†’ Logs are coming from Loki.
|~ "HTTP/1.1" â†’ Filters logs that contain "HTTP/1.1".
|~ "5\\d{2}" â†’ Filters logs that contain HTTP status codes 500-599 (server errors).
rate(...[5m]) â†’ Calculates the rate of these errors over the last 5 minutes.
sum(...) by (instance) > 5 â†’ If the number of 5xx errors exceeds 5 per instance, the alert triggers.
Use Case:
Indicates that the application is experiencing frequent server errors.
Needs immediate attention, as it affects end-user experience.
2ï¸âƒ£ ApplicationLogErrors (Warning Severity)
Purpose:
Detects a high number of ERROR messages in application logs.
The alert fires if there are more than 10 ERROR logs in the last 5 minutes.
Expression Breakdown:
yaml
Copy
Edit
count_over_time({job="loki"} |~ "ERROR" [5m]) by (instance) > 10
job="loki" â†’ Logs are coming from Loki.
|~ "ERROR" â†’ Filters logs that contain "ERROR".
count_over_time(...[5m]) â†’ Counts the occurrences of "ERROR" in logs over 5 minutes.
by (instance) > 10 â†’ If more than 10 errors are logged per instance in the last 5 minutes, the alert fires.
Use Case:
Helps detect application errors that may indicate failing services, database issues, or unhandled exceptions.
Severity is warning (not critical), meaning it needs monitoring but is not an emergency.
3ï¸âƒ£ ApplicationLogWarnings (Warning Severity)
Purpose:
Detects a high number of WARNING messages in application logs.
The alert fires if there are more than 20 WARNING logs in the last 5 minutes.
Expression Breakdown:
yaml
Copy
Edit
count_over_time({job="loki"} |~ "WARNING" [5m]) by (instance) > 20
job="loki" â†’ Logs are coming from Loki.
|~ "WARNING" â†’ Filters logs that contain "WARNING".
count_over_time(...[5m]) â†’ Counts the occurrences of "WARNING" logs over 5 minutes.
by (instance) > 20 â†’ If more than 20 warnings are logged per instance in the last 5 minutes, the alert fires.
Use Case:
Warnings may indicate potential issues, such as performance degradation, minor failures, or deprecations.
Needs investigation but isnâ€™t as urgent as errors or 5xx failures.
Summary Table
Alert Name	Condition (within 5 min)	Severity	Purpose
HTTP5xxErrors	More than 5 HTTP 5xx errors	ðŸ”´ Critical	Detects server errors (5xx) that affect users
ApplicationLogErrors	More than 10 ERROR logs	ðŸŸ  Warning	Detects high error rates in application logs
ApplicationLogWarnings	More than 20 WARNING logs	ðŸŸ  Warning	Detects high warning counts, indicating potential issues
