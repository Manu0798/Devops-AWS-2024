# Install Node_export, Blackbox, Alertmanager, Loki, promotail and grafana

#!/bin/bash

# Update package list
echo "Updating package list..."
sudo apt-get update -y


# Install Java
sudo apt-get update
sudo apt-get install fontconfig openjdk-17-jre -y


# Monitoring Tools
# Update the packages
sudo apt update && sudo apt upgrade -y
sudo apt install -y net-tools wget curl unzip apt-transport-https

# Install AWS_CLI
sudo apt-get update -y
sudo apt-get install -y curl unzip
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install



# Create staging directory if not exists
mkdir -p ~/staging

# Fetch Application Logs
cat <<'EOF' | tee ~/staging/application_log_fetcher.sh
#!/bin/bash
LOG_GROUPS=(
  "/aws/apprunner/staging-Backend-01/8a9d8f5e0c2a452bbf4cdd7369a29a3c/application"
)
LOG_FILE="/var/log/staging_application_cloudwatch.log"

mkdir -p $(dirname "$LOG_FILE")
sudo touch "$LOG_FILE"
sudo chmod 666 "$LOG_FILE"

for LOG_GROUP in "${LOG_GROUPS[@]}"; do
  LOG_STREAM_NAME=$(aws logs describe-log-streams --log-group-name "$LOG_GROUP" \
    --order-by "LastEventTime" --descending --query 'logStreams[0].logStreamName' --output text 2>/dev/null)

  if [[ -z "$LOG_STREAM_NAME" || "$LOG_STREAM_NAME" == "None" ]]; then
    echo "$(date) - No log stream found in $LOG_GROUP" | sudo tee -a "$LOG_FILE"
    continue
  fi

  LOGS=$(aws logs get-log-events --log-group-name "$LOG_GROUP" --log-stream-name "$LOG_STREAM_NAME" \
    --limit 100 --query 'events[*].message' --output text 2>/dev/null)

  if [[ -z "$LOGS" ]]; then
    echo "$(date) - No new logs found in stream $LOG_STREAM_NAME of $LOG_GROUP" | sudo tee -a "$LOG_FILE"
  else
    echo "[$LOG_GROUP] $LOGS" | sudo tee -a "$LOG_FILE"
    echo "$(date) - Logs updated from $LOG_STREAM_NAME of $LOG_GROUP" | sudo tee -a "$LOG_FILE"
  fi
done
EOF

# Fetch Service Logs
cat <<'EOF' | tee ~/staging/service_log_fetcher.sh
#!/bin/bash
LOG_GROUPS=(
  "/aws/apprunner/staging-Backend-01/8a9d8f5e0c2a452bbf4cdd7369a29a3c/service"
)
LOG_FILE="/var/log/staging_service_cloudwatch.log"

mkdir -p $(dirname "$LOG_FILE")
sudo touch "$LOG_FILE"
sudo chmod 666 "$LOG_FILE"

for LOG_GROUP in "${LOG_GROUPS[@]}"; do
  LOG_STREAM_NAME=$(aws logs describe-log-streams --log-group-name "$LOG_GROUP" \
    --order-by "LastEventTime" --descending --query 'logStreams[0].logStreamName' --output text 2>/dev/null)

  if [[ -z "$LOG_STREAM_NAME" || "$LOG_STREAM_NAME" == "None" ]]; then
    echo "$(date) - No log stream found in $LOG_GROUP" | sudo tee -a "$LOG_FILE"
    continue
  fi

  LOGS=$(aws logs get-log-events --log-group-name "$LOG_GROUP" --log-stream-name "$LOG_STREAM_NAME" \
    --limit 100 --query 'events[*].message' --output text 2>/dev/null)

  if [[ -z "$LOGS" ]]; then
    echo "$(date) - No new logs found in stream $LOG_STREAM_NAME of $LOG_GROUP" | sudo tee -a "$LOG_FILE"
  else
    echo "[$LOG_GROUP] $LOGS" | sudo tee -a "$LOG_FILE"
    echo "$(date) - Logs updated from $LOG_STREAM_NAME of $LOG_GROUP" | sudo tee -a "$LOG_FILE"
  fi
done
EOF

# Make scripts executable
chmod +x ~/staging/application_log_fetcher.sh
chmod +x ~/staging/service_log_fetcher.sh

# Add to crontab for periodic execution
(crontab -l 2>/dev/null; echo "* * * * * /home/ubuntu/staging/application_log_fetcher.sh") | crontab -
(crontab -l 2>/dev/null; echo "* * * * * /home/ubuntu/staging/service_log_fetcher.sh") | crontab -

echo "Setup complete! Cron jobs added."



# # Create Production directory if not exists
# mkdir -p ~/production

# # Fetch Application Logs
# cat <<'EOF' | tee ~/production/application_log_fetcher.sh
# #!/bin/bash
# LOG_GROUPS=(
#   "/aws/apprunner/production-Backend-01/a5bafd5c192e42b29dd3a0c58efcc3c0/application"
# )
# LOG_FILE="/var/log/production_application_cloudwatch.log"

# mkdir -p $(dirname "$LOG_FILE")
# sudo touch "$LOG_FILE"
# sudo chmod 666 "$LOG_FILE"

# for LOG_GROUP in "${LOG_GROUPS[@]}"; do
#   LOG_STREAM_NAME=$(aws logs describe-log-streams --log-group-name "$LOG_GROUP" \
#     --order-by "LastEventTime" --descending --query 'logStreams[0].logStreamName' --output text 2>/dev/null)

#   if [[ -z "$LOG_STREAM_NAME" || "$LOG_STREAM_NAME" == "None" ]]; then
#     echo "$(date) - No log stream found in $LOG_GROUP" | sudo tee -a "$LOG_FILE"
#     continue
#   fi

#   LOGS=$(aws logs get-log-events --log-group-name "$LOG_GROUP" --log-stream-name "$LOG_STREAM_NAME" \
#     --limit 100 --query 'events[*].message' --output text 2>/dev/null)

#   if [[ -z "$LOGS" ]]; then
#     echo "$(date) - No new logs found in stream $LOG_STREAM_NAME of $LOG_GROUP" | sudo tee -a "$LOG_FILE"
#   else
#     echo "[$LOG_GROUP] $LOGS" | sudo tee -a "$LOG_FILE"
#     echo "$(date) - Logs updated from $LOG_STREAM_NAME of $LOG_GROUP" | sudo tee -a "$LOG_FILE"
#   fi
# done
# EOF

# # Fetch Service Logs
# cat <<'EOF' | tee ~/production/service_log_fetcher.sh
# #!/bin/bash
# LOG_GROUPS=(
#   "/aws/apprunner/production-Backend-01/a5bafd5c192e42b29dd3a0c58efcc3c0/service"
# )
# LOG_FILE="/var/log/production_service_cloudwatch.log"

# mkdir -p $(dirname "$LOG_FILE")
# sudo touch "$LOG_FILE"
# sudo chmod 666 "$LOG_FILE"

# for LOG_GROUP in "${LOG_GROUPS[@]}"; do
#   LOG_STREAM_NAME=$(aws logs describe-log-streams --log-group-name "$LOG_GROUP" \
#     --order-by "LastEventTime" --descending --query 'logStreams[0].logStreamName' --output text 2>/dev/null)

#   if [[ -z "$LOG_STREAM_NAME" || "$LOG_STREAM_NAME" == "None" ]]; then
#     echo "$(date) - No log stream found in $LOG_GROUP" | sudo tee -a "$LOG_FILE"
#     continue
#   fi

#   LOGS=$(aws logs get-log-events --log-group-name "$LOG_GROUP" --log-stream-name "$LOG_STREAM_NAME" \
#     --limit 100 --query 'events[*].message' --output text 2>/dev/null)

#   if [[ -z "$LOGS" ]]; then
#     echo "$(date) - No new logs found in stream $LOG_STREAM_NAME of $LOG_GROUP" | sudo tee -a "$LOG_FILE"
#   else
#     echo "[$LOG_GROUP] $LOGS" | sudo tee -a "$LOG_FILE"
#     echo "$(date) - Logs updated from $LOG_STREAM_NAME of $LOG_GROUP" | sudo tee -a "$LOG_FILE"
#   fi
# done
# EOF

# # Make scripts executable
# chmod +x ~/production/application_log_fetcher.sh
# chmod +x ~/production/service_log_fetcher.sh

# # Add to crontab for periodic execution
# (crontab -l 2>/dev/null; echo "* * * * * /home/ubuntu/production/application_log_fetcher.sh") | crontab -
# (crontab -l 2>/dev/null; echo "* * * * * /home/ubuntu/production/service_log_fetcher.sh") | crontab -

# echo "Setup complete! Cron jobs added."




# Variables
NODE_EXPORTER_VERSION="1.9.0"
BLACKBOX_EXPORTER_VERSION="0.26.0"
ALERTMANAGER_VERSION="v0.27.0"
LOKI_VERSION="v3.4.2"
PROMTAIL_VERSION="latest"

# Install Node Exporter
wget https://github.com/prometheus/node_exporter/releases/download/v$NODE_EXPORTER_VERSION/node_exporter-$NODE_EXPORTER_VERSION.linux-amd64.tar.gz
tar -xvf node_exporter-$NODE_EXPORTER_VERSION.linux-amd64.tar.gz
rm node_exporter-$NODE_EXPORTER_VERSION.linux-amd64.tar.gz
sudo mkdir -p /opt/node_exporter
sudo mv node_exporter-$NODE_EXPORTER_VERSION.linux-amd64 /opt/node_exporter
sudo useradd --no-create-home --shell /bin/false node_exporter
sudo chown -R node_exporter:node_exporter /opt/node_exporter/
sudo chmod -R 755 /opt/node_exporter/

cat <<EOF | sudo tee /etc/systemd/system/node_exporter.service
[Unit]
Description=Node Exporter
After=network.target

[Service]
User=node_exporter
Group=node_exporter
Type=simple
ExecStart=/opt/node_exporter/node_exporter-$NODE_EXPORTER_VERSION.linux-amd64/node_exporter

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload
sudo systemctl enable node_exporter
sudo systemctl start node_exporter


# Install Blackbox Exporter
wget https://github.com/prometheus/blackbox_exporter/releases/download/v$BLACKBOX_EXPORTER_VERSION/blackbox_exporter-$BLACKBOX_EXPORTER_VERSION.linux-amd64.tar.gz
tar -xvf blackbox_exporter-$BLACKBOX_EXPORTER_VERSION.linux-amd64.tar.gz
rm blackbox_exporter-$BLACKBOX_EXPORTER_VERSION.linux-amd64.tar.gz
sudo mkdir -p /opt/blackbox_exporter
sudo mv blackbox_exporter-$BLACKBOX_EXPORTER_VERSION.linux-amd64 /opt/blackbox_exporter/
sudo useradd --no-create-home --shell /bin/false blackbox_exporter
sudo chown -R blackbox_exporter:blackbox_exporter /opt/blackbox_exporter
sudo chmod -R 755 /opt/blackbox_exporter/
sudo chmod 644 /opt/blackbox_exporter/blackbox_exporter-0.26.0.linux-amd64/blackbox.yml


cat <<EOF | sudo tee /etc/systemd/system/blackbox_exporter.service
[Unit]
Description=Blackbox Exporter
After=network.target

[Service]
User=blackbox_exporter
Group=blackbox_exporter
Type=simple
ExecStart=/opt/blackbox_exporter/blackbox_exporter-$BLACKBOX_EXPORTER_VERSION.linux-amd64/blackbox_exporter --config.file=/opt/blackbox_exporter/blackbox_exporter-0.26.0.linux-amd64/blackbox.yml
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload
sudo systemctl enable blackbox_exporter
sudo systemctl start blackbox_exporter


# Install Alert Manager
wget https://github.com/prometheus/alertmanager/releases/download/v0.27.0/alertmanager-0.27.0.linux-amd64.tar.gz
tar -xvf alertmanager-0.27.0.linux-amd64.tar.gz
rm alertmanager-0.27.0.linux-amd64.tar.gz
sudo mkdir -p /opt/alertmanager
sudo mv alertmanager-0.27.0.linux-amd64 /opt/alertmanager/
sudo useradd --no-create-home --shell /bin/false alertmanager
sudo mkdir /var/lib/alertmanager
sudo chown alertmanager:alertmanager /var/lib/alertmanager
sudo chown -R alertmanager:alertmanager /opt/alertmanager
sudo chmod -R 755 /opt/alertmanager

cat <<EOF | sudo tee /opt/alertmanager/alertmanager-0.27.0.linux-amd64/alertmanager.yml
route:
  group_by:
    - alertname
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 1h
  receiver: email-notifications
receivers:
  - name: email-notifications
    email_configs:
      - to: manojvarmapotthutri@gmail.com
        from: varmapotthuri4@gmail.com
        smarthost: smtp.gmail.com:587
        auth_username: varmapotthuri4@gmail.com
        auth_identity: varmapotthuri4@gmail.com
        auth_password: sojtftvdabttfgdu
        send_resolved: true
EOF

cat <<EOF | sudo tee /etc/systemd/system/alertmanager.service
[Unit]
Description=Alertmanager
After=network.target

[Service]
User=alertmanager
Group=alertmanager
Type=simple
ExecStart=/opt/alertmanager/alertmanager-0.27.0.linux-amd64/alertmanager --config.file=/opt/alertmanager/alertmanager-0.27.0.linux-amd64/alertmanager.yml --storage.path=/var/lib/alertmanager

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload
sudo systemctl enable alertmanager
sudo systemctl start alertmanager
ps aux | grep alertmanager
sudo netstat -tulnp | grep 9093


# Install Loki
echo "Installing Loki..."
wget https://github.com/grafana/loki/releases/download/v3.4.2/loki-linux-amd64.zip -O ~/loki-server.zip
unzip ~/loki-server.zip -d ~/loki-server
sudo mv ~/loki-server/loki-linux-amd64 /usr/local/bin/loki
sudo chmod +x /usr/local/bin/loki

# Configure Loki
cat <<EOF | sudo tee /etc/loki-config.yml
auth_enabled: false
server:
  http_listen_port: 3100
common:
  path_prefix: /var/loki
ingester:
  lifecycler:
    address: 127.0.0.1
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
  chunk_idle_period: 5m
  chunk_retain_period: 30s
schema_config:
  configs:
    - from: 2022-06-01
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h
storage_config:
  boltdb_shipper:
    active_index_directory: /var/loki/index
    cache_location: /var/loki/cache
  filesystem:
    directory: /var/loki/chunks
limits_config:
  reject_old_samples: true
  reject_old_samples_max_age: 168h
  allow_structured_metadata: false
compactor:
  working_directory: /var/loki/compactor
table_manager:
  retention_deletes_enabled: true
  retention_period: 168h
EOF

# Create Loki service
cat <<EOF | sudo tee /etc/systemd/system/loki.service
[Unit]
Description=Loki Log Aggregation System
After=network.target

[Service]
ExecStart=/usr/local/bin/loki -config.file=/etc/loki-config.yml
Restart=always
User=root

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload
sudo systemctl enable loki
sudo systemctl start loki



# Install Promtail
echo "Installing Promtail..."
wget https://github.com/grafana/loki/releases/latest/download/promtail-linux-amd64.zip -O ~/promtail.zip
sudo apt update && sudo apt install unzip -y
unzip ~/promtail.zip -d ~/promtail
sudo mv ~/promtail/promtail-linux-amd64 /usr/local/bin/promtail
sudo chmod +x /usr/local/bin/promtail

# Configure Promtail
cat <<EOF | sudo tee /etc/promtail-config.yml
server:
  http_listen_port: 9080
  grpc_listen_port: 0
positions:
  filename: /tmp/positions.yaml
clients:
  - url: http://localhost:3100/loki/api/v1/push
scrape_configs:
  - job_name: system
    static_configs:
      - targets:
          - localhost
        labels:
          job: varlogs
          __path__: /var/log/*log
          stream: stdout

  - job_name: stating_application_logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: staging_application_logs
          __path__: /var/log/staging_application_cloudwatch.log

  - job_name: stating_service_logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: staging_service_logs
          __path__: /var/log/staging_service_cloudwatch.log

EOF

# Create Promtail service
cat <<EOF | sudo tee /etc/systemd/system/promtail.service
[Unit]
Description=Promtail
After=network.target

[Service]
Type=simple
ExecStart=/usr/local/bin/promtail -config.file=/etc/promtail-config.yml
Restart=always
User=root

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload
sudo systemctl enable promtail
sudo systemctl start promtail

# Once Need to do restart loki
sudo systemctl daemon-reload
sudo systemctl restart loki


# Install Grafana
echo "Installing Grafana..."
sudo apt-get install -y adduser libfontconfig1 musl
wget https://dl.grafana.com/enterprise/release/grafana-enterprise_11.5.2_amd64.deb
sudo dpkg -i grafana-enterprise_11.5.2_amd64.deb

# Reload systemd and enable Grafana
sudo /bin/systemctl daemon-reload
sudo /bin/systemctl enable grafana-server
sudo /bin/systemctl start grafana-server
# sudo /bin/systemctl status grafana-server

# Configure Grafana for email notifications
echo "Configuring Grafana for email notifications..."
cat <<EOF | sudo tee -a /etc/grafana/grafana.ini
[smtp]
enabled = true
host = smtp.gmail.com:587
user = varmapotthuri4@gmail.com
# If the password contains # or ; you have to wrap it with triple quotes. Ex """#password;"""
password = sojtftvdabttfgdu
skip_verify = true
from_address = varmapotthuri4@gmail.com
from_name = Grafana
EOF

# Restart Grafana to apply changes
sudo /bin/systemctl daemon-reload
sudo /bin/systemctl restart grafana-server



# After Install above tools install promethous
-> Create one file in terminal and execute the below script

#!/bin/bash

# Update package list
echo "Updating package list..."
sudo apt-get update -y

# Install Java
echo "Installing Java..."
sudo apt-get install -y fontconfig openjdk-17-jre

# Monitoring Tools
echo "Installing monitoring tools..."
sudo apt-get update && sudo apt-get upgrade -y
sudo apt-get install -y net-tools wget curl unzip apt-transport-https

# Install Prometheus
echo "Installing Prometheus..."
wget https://github.com/prometheus/prometheus/releases/download/v3.2.1/prometheus-3.2.1.linux-amd64.tar.gz
tar -xvf prometheus-3.2.1.linux-amd64.tar.gz
rm prometheus-3.2.1.linux-amd64.tar.gz
sudo mkdir -p /opt/prometheus
sudo mv prometheus-3.2.1.linux-amd64 /opt/prometheus/
sudo useradd --no-create-home --shell /bin/false prometheus
sudo mkdir /var/lib/prometheus

# Configure Prometheus alert rules
echo "Configuring Prometheus alert rules..."
cat <<EOF | sudo tee /opt/prometheus/prometheus-3.2.1.linux-amd64/alert_rules.yml
groups:
  - name: alert_rules
    rules:
      - alert: InstanceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: Endpoint {{ $labels.instance }} down
          description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minute."

      - alert: WebsiteDown
        expr: probe_success == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: Website down
          description: The website at {{ $labels.instance }} is down.

      - alert: HostOutOfMemory
        expr: (node_memory_MemAvailable / node_memory_MemTotal * 100) < 25
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Host out of memory (instance {{ $labels.instance }})
          description: |-
            Node memory is filling up (< 25% left)
              VALUE = {{ $value }}
              LABELS: {{ $labels }}

      - alert: HostOutOfDiskSpace
        expr: (node_filesystem_avail{mountpoint="/"} * 100) / node_filesystem_size{mountpoint="/"} < 50
        for: 1s
        labels:
          severity: warning
        annotations:
          summary: Host out of disk space (instance {{ $labels.instance }})
          description: |-
            Disk is almost full (< 50% left)
              VALUE = {{ $value }}
              LABELS: {{ $labels }}

      - alert: HostHighCpuLoad
        expr: (1 - avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m]))) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Host high CPU load (instance {{ $labels.instance }})
          description: |-
            CPU load is > 80%
              VALUE = {{ $value }}
              LABELS: {{ $labels }}

      - alert: ServiceUnavailable
        expr: up{job="node_exporter"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: Service Unavailable (instance {{ $labels.instance }})
          description: |-
            The service {{ $labels.job }} is not available
              VALUE = {{ $value }}
              LABELS: {{ $labels }}

      - alert: HighMemoryUsage
        expr: (node_memory_Active / node_memory_MemTotal) * 100 > 90
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: High Memory Usage (instance {{ $labels.instance }})
          description: |-
            Memory usage is > 90%
              VALUE = {{ $value }}
              LABELS: {{ $labels }}

      - alert: FileSystemFull
        expr: (node_filesystem_avail / node_filesystem_size) * 100 < 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: File System Almost Full (instance {{ $labels.instance }})
          description: |-
            File system has < 10% free space
              VALUE = {{ $value }}
              LABELS: {{ $labels }}

      - alert: HighCPUUsage
        expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: High CPU Usage on {{ $labels.instance }}
          description: "CPU usage is above 80% for more than 5 minutes."

      - alert: HighSystemLoad
        expr: node_load1 > 0.9 * count(node_cpu_seconds_total{mode="user"})
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High System Load on {{ $labels.instance }}
          description: "System load is above 90% for more than 5 minutes."

      - alert: HighSwapUsage
        expr: (node_memory_SwapTotal_bytes - node_memory_SwapFree_bytes) / node_memory_SwapTotal_bytes * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High Swap Usage on {{ $labels.instance }}
          description: "Swap usage is above 80% for more than 5 minutes."

      - alert: HighDiskUsage
        expr: (node_filesystem_size_bytes{mountpoint="/"} - node_filesystem_free_bytes{mountpoint="/"}) / node_filesystem_size_bytes{mountpoint="/"} * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High Disk Usage on {{ $labels.instance }}
          description: "Root filesystem usage is above 85% for more than 5 minutes."

  - name: docker-container-alerts
    rules:
      - alert: DockerDiskUsageHigh
        expr: node_filesystem_free_bytes{mountpoint="/var/lib/docker"} / node_filesystem_size_bytes{mountpoint="/var/lib/docker"} * 100 < 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Docker Disk Usage High"
          description: "Docker disk usage is above 90%, remaining space is less than 10%."

      - alert: DockerContainerHighMemory
        expr: container_memory_usage_bytes{container_label_com_docker_compose_service!=""} / container_spec_memory_limit_bytes * 100 > 80
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High Memory Usage in Docker Container"
          description: "Container {{ .container_label_com_docker_compose_service }} is using more than 80% of its allocated memory."

      - alert: DockerContainerHighCPU
        expr: rate(container_cpu_usage_seconds_total{container_label_com_docker_compose_service!=""}[5m]) * 100 > 80
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High CPU Usage in Docker Container"
          description: "Container {{ .container_label_com_docker_compose_service }} is using more than 80% CPU."

      - alert: DockerContainerRestarting
        expr: increase(container_restart_count{container_label_com_docker_compose_service!=""}[10m]) > 3
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Docker Container Restarting Frequently"
          description: "Container {{ .container_label_com_docker_compose_service }} has restarted more than 3 times in 10 minutes."

      - alert: DockerContainerStopped
        expr: sum by (state) (engine_daemon_container_states_containers{state="stopped"}) > 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Stopped Container Alert"
          description: "One or more Docker containers are in a stopped state for more than 2 minutes."

      - alert: DockerContainerPaused
        expr: sum by (state) (engine_daemon_container_states_containers{state="paused"}) > 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Paused Container Alert"
          description: "One or more Docker containers are in a paused state."

      - alert: NoRunningContainers
        expr: sum by (state) (engine_daemon_container_states_containers{state="running"}) == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "No Running Containers"
          description: "There are no running containers on the host."

  - name: ssl_alerts
    rules:
      - alert: SSLCertificateExpiring
        expr: ssl_cert_expires_in < 30 * 24 * 60 * 60
        for: 1d
        labels:
          severity: warning
        annotations:
          summary: SSL certificate expiring soon ({{ $labels.instance }})
          description: "The SSL certificate expires in less than 30 days."

      - alert: SSLCertificateExpired
        expr: ssl_cert_expires_in <= 0
        for: 1d
        labels:
          severity: critical
        annotations:
          summary: SSL certificate expired ({{ $labels.instance }})
          description: "The SSL certificate has expired."

  - name: latency_alerts
    rules:
      - alert: HighApplicationLatency
        expr: histogram_quantile(0.99, sum by (instance, job) (rate(application_latency_seconds_bucket[5m]))) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High application latency ({{ $labels.instance }})
          description: "The 99th percentile latency is above 500ms."

      - alert: CriticalApplicationLatency
        expr: histogram_quantile(0.99, sum by (instance, job) (rate(application_latency_seconds_bucket[5m]))) > 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: Critical application latency ({{ $labels.instance }})
          description: "The 99th percentile latency is above 1000ms."

  - name: Alerts of HTTP Duration phase
    rules:
      - alert: SlowTCPConnection
        expr: probe_http_duration_seconds{phase="connect"} > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High TCP connection time to website {{ $labels.instance }}"
          description: "The TCP connection time to the website {{ $labels.instance }} has exceeded 100ms for 2 minutes. This may indicate network issues or delays in the load balancer."

      - alert: SlowBackendProcessing
        expr: probe_http_duration_seconds{phase="processing"} > 0.05
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High backend processing time for website {{ $labels.instance }}"
          description: "The backend response time for website {{ $labels.instance }} has exceeded 50ms for 2 minutes. This may impact application performance, causing delays in checkout, product loading, or search functionalities."

      - alert: DNSResolutionDelay
        expr: probe_http_duration_seconds{phase="resolve"} > 0.05
        for: 2m
        labels:
          severity: info
        annotations:
          summary: "DNS resolution delay for website {{ $labels.instance }}"
          description: "DNS resolution for website {{ $labels.instance }} has taken longer than expected, with a lookup time exceeding 50ms for 2 minutes. Consider optimizing DNS configurations or caching DNS results."

      - alert: TLSHandshakeLatency
        expr: probe_http_duration_seconds{phase="tls"} > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "TLS handshake time is high for website {{ $labels.instance }}"
          description: "TLS negotiation for website {{ $labels.instance }} is taking longer than 100ms. This may impact secure connections and performance."

      - alert: SlowHTTPTransfer
        expr: probe_http_duration_seconds{phase="transfer"} > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High response transfer time for website {{ $labels.instance }}"
          description: "HTTP response transfer for website {{ $labels.instance }} is slow (>100ms). This may be due to large payloads or slow backend."


  - name: Alerts of High Probe Duration
    rules:
      - alert: HighProbeDuration
        expr: probe_duration_seconds > 0.3
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High probe duration detected for website {{ $labels.instance }}"
          description: "Probe duration for website {{ $labels.instance }} is high (>300ms). This may indicate network latency or slow server response."
EOF

# Set ownership and permissions
echo "Setting ownership and permissions..."
sudo chown prometheus:prometheus /var/lib/prometheus
sudo chown -R prometheus:prometheus /opt/prometheus
sudo chmod -R 755 /opt/prometheus

# Configure Prometheus
echo "Configuring Prometheus..."
cat <<EOF | sudo tee /opt/prometheus/prometheus-3.2.1.linux-amd64/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - "localhost:9093"

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: "prometheus"
    static_configs:
      - targets:
          - "localhost:9090"

  - job_name: "node_exporter"
    static_configs:
      - targets:
          - "localhost:9100"
          - "13.203.149.189:9100"

  - job_name: "docker"
    static_configs:
      - targets:
          - "13.203.149.189:9323"

  - job_name: "blackbox"
    metrics_path: /probe
    params:
      module: [http_2xx]
    static_configs:
      - targets:
          - "https://frissly.com"
          - "https://admin.frissly.com"
    relabel_configs:
      - source_labels: [address]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: address
        replacement: "localhost:9115"
EOF

# Create Prometheus service
echo "Creating Prometheus service..."
cat <<EOF | sudo tee /etc/systemd/system/prometheus.service
[Unit]
Description=Prometheus
After=network.target

[Service]
User=prometheus
Group=prometheus
Type=simple
ExecStart=/opt/prometheus/prometheus-3.2.1.linux-amd64/prometheus --config.file=/opt/prometheus/prometheus-3.2.1.linux-amd64/prometheus.yml --storage.tsdb.path=/var/lib/prometheus

[Install]
WantedBy=multi-user.target
EOF

# Reload systemd and start Prometheus
echo "Reloading systemd and starting Prometheus..."
sudo systemctl daemon-reload
sudo systemctl enable prometheus
sudo systemctl start prometheus

# Verify Prometheus is running
echo "Verifying Prometheus is running..."
ps aux | grep prometheus
sudo netstat -tulnp | grep 9090

echo "Prometheus installation and configuration complete!"


-> After execute the above script , if blackbox exporter not appear in promethous you will copy below data and pate this path 
-> sudo vi /opt/prometheus/prometheus-3.2.1.linux-amd64/prometheus.yml

global:
  scrape_interval: 15s
  evaluation_interval: 15s

alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - "localhost:9093"

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: "prometheus"
    static_configs:
      - targets:
          - "localhost:9090"

  - job_name: "monitor_node_exporter"
    static_configs:
      - targets:
          - "localhost:9100"

  - job_name: "blackbox"
    metrics_path: /probe
    params:
      module: [http_2xx]
    static_configs:
      - targets:
          - "https://4qft2jsmnh.ap-south-1.awsapprunner.com"
          - "https://frissly.com"
          - "https://admin.frissly.com"
          - "https://m.frissly.com"
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: "localhost:9115"

-> sudo systemctl daemon-reload
-> sudo systemctl restart prometheus
-> ps aux | grep prometheus
-> sudo netstat -tulnp | grep 9090




# If want to install cloud watch exporter
# Download the correct "with dependencies" JAR (replace link with latest from GitHub if needed)
-> sudo wget https://repo1.maven.org/maven2/io/prometheus/cloudwatch/cloudwatch_exporter/0.15.4/cloudwatch_exporter-0.15.4-jar-with-dependencies.jar

# Rename for convenience
-> sudo mv cloudwatch_exporter-0.15.4-jar-with-dependencies.jar cloudwatch_exporter-0.15.4.jar
-> sudo mkdir -p /opt/cloudwatch_exporter
-> sudo mv cloudwatch_exporter-0.15.4.jar /opt/cloudwatch_exporter/
-> sudo vi /etc/cloudwatch_exporter.yml

region: ap-south-1

metrics:
  - aws_namespace: AWS/AppRunner
    aws_metric_name: RequestCount
    dimensions: [ServiceId]
    statistics: [Sum]
    period_seconds: 300
    range_seconds: 600
    delay_seconds: 60

  - aws_namespace: AWS/AppRunner
    aws_metric_name: HTTPCode_Target_2XX_Count
    dimensions: [ServiceId]
    statistics: [Sum]
    period_seconds: 300
    range_seconds: 600
    delay_seconds: 60

  - aws_namespace: AWS/AppRunner
    aws_metric_name: HTTPCode_Target_4XX_Count
    dimensions: [ServiceId]
    statistics: [Sum]
    period_seconds: 300
    range_seconds: 600
    delay_seconds: 60

  - aws_namespace: AWS/AppRunner
    aws_metric_name: HTTPCode_Target_5XX_Count
    dimensions: [ServiceId]
    statistics: [Sum]
    period_seconds: 300
    range_seconds: 600
    delay_seconds: 60

  - aws_namespace: AWS/AppRunner
    aws_metric_name: Latency
    dimensions: [ServiceId]
    statistics: [Average]
    period_seconds: 300
    range_seconds: 600
    delay_seconds: 60

  - aws_namespace: AWS/AppRunner
    aws_metric_name: ActiveInstances
    dimensions: [ServiceId]
    statistics: [Average]
    period_seconds: 300
    range_seconds: 600
    delay_seconds: 60

  - aws_namespace: AWS/AppRunner
    aws_metric_name: CPUUtilization
    dimensions: [ServiceId]
    statistics: [Average]
    period_seconds: 300
    range_seconds: 600
    delay_seconds: 60

  - aws_namespace: AWS/AppRunner
    aws_metric_name: MemoryUtilization
    dimensions: [ServiceId]
    statistics: [Average]
    period_seconds: 300
    range_seconds: 600
    delay_seconds: 60

  - aws_namespace: AWS/AppRunner
    aws_metric_name: Concurrency
    dimensions: [ServiceId]
    statistics: [Average]
    period_seconds: 300
    range_seconds: 600
    delay_seconds: 60

-> sudo vi /etc/systemd/system/cloudwatch_exporter.service

[Unit]
Description=CloudWatch Exporter
After=network.target

[Service]
User=ubuntu
ExecStart=/usr/bin/java -jar /opt/cloudwatch_exporter/cloudwatch_exporter-0.15.4.jar 9106 /etc/cloudwatch_exporter.yml
SuccessExitStatus=143

[Install]
WantedBy=multi-user.target

-> sudo apt install default-jre
-> sudo systemctl daemon-reexec
-> sudo systemctl daemon-reload
-> sudo systemctl enable cloudwatch_exporter
-> sudo systemctl start cloudwatch_exporter
-> sudo systemctl status cloudwatch_exporter
-> sudo journalctl -u cloudwatch_exporter -f

# For restart
-> sudo systemctl restart cloudwatch_exporter








# If promethous it could not start you will use below alerts
groups:
  - name: alert_rules
    rules:
      - alert: InstanceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: Endpoint {{ $labels.instance }} down
          description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minute."

      - alert: WebsiteDown
        expr: probe_success == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: Website down
          description: The website at {{ $labels.instance }} is down.

      - alert: HostOutOfMemory
        expr: (node_memory_MemAvailable / node_memory_MemTotal * 100) < 25
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Host out of memory (instance {{ $labels.instance }})
          description: |-
            Node memory is filling up (< 25% left)
              VALUE = {{ $value }}
              LABELS: {{ $labels }}

      - alert: HostOutOfDiskSpace
        expr: (node_filesystem_avail{mountpoint="/"} * 100) / node_filesystem_size{mountpoint="/"} < 50
        for: 1s
        labels:
          severity: warning
        annotations:
          summary: Host out of disk space (instance {{ $labels.instance }})
          description: |-
            Disk is almost full (< 50% left)
              VALUE = {{ $value }}
              LABELS: {{ $labels }}

      - alert: HostHighCpuLoad
        expr: (1 - avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m]))) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Host high CPU load (instance {{ $labels.instance }})
          description: |-
            CPU load is > 80%
              VALUE = {{ $value }}
              LABELS: {{ $labels }}

      - alert: ServiceUnavailable
        expr: up{job="node_exporter"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: Service Unavailable (instance {{ $labels.instance }})
          description: |-
            The service {{ $labels.job }} is not available
              VALUE = {{ $value }}
              LABELS: {{ $labels }}

      - alert: HighMemoryUsage
        expr: (node_memory_Active / node_memory_MemTotal) * 100 > 90
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: High Memory Usage (instance {{ $labels.instance }})
          description: |-
            Memory usage is > 90%
              VALUE = {{ $value }}
              LABELS: {{ $labels }}

      - alert: FileSystemFull
        expr: (node_filesystem_avail / node_filesystem_size) * 100 < 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: File System Almost Full (instance {{ $labels.instance }})
          description: |-
            File system has < 10% free space
              VALUE = {{ $value }}
              LABELS: {{ $labels }}

      - alert: HighCPUUsage
        expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: High CPU Usage on {{ $labels.instance }}
          description: "CPU usage is above 80% for more than 5 minutes."

      - alert: HighSystemLoad
        expr: node_load1 > 0.9 * count(node_cpu_seconds_total{mode="user"})
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High System Load on {{ $labels.instance }}
          description: "System load is above 90% for more than 5 minutes."

      - alert: HighSwapUsage
        expr: (node_memory_SwapTotal_bytes - node_memory_SwapFree_bytes) / node_memory_SwapTotal_bytes * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High Swap Usage on {{ $labels.instance }}
          description: "Swap usage is above 80% for more than 5 minutes."

      - alert: HighDiskUsage
        expr: (node_filesystem_size_bytes{mountpoint="/"} - node_filesystem_free_bytes{mountpoint="/"}) / node_filesystem_size_bytes{mountpoint="/"} * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High Disk Usage on {{ $labels.instance }}
          description: "Root filesystem usage is above 85% for more than 5 minutes."

  - name: docker-container-alerts
    rules:
      # 🚨 Alert: Docker Disk Usage High
      - alert: DockerDiskUsageHigh
        expr: node_filesystem_free_bytes{mountpoint="/var/lib/docker"} / node_filesystem_size_bytes{mountpoint="/var/lib/docker"} * 100 < 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Docker Disk Usage High"
          description: "Docker disk usage is above 90%, remaining space is less than 10%."

      # 🚨 Alert: High Memory Usage in a Container
      - alert: DockerContainerHighMemory
        expr: container_memory_usage_bytes{container_label_com_docker_compose_service!=""} / container_spec_memory_limit_bytes * 100 > 80
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High Memory Usage in Docker Container"
          description: "Container {{ .container_label_com_docker_compose_service }} is using more than 80% of its allocated memory."

      # 🚨 Alert: High CPU Usage in a Container
      - alert: DockerContainerHighCPU
        expr: rate(container_cpu_usage_seconds_total{container_label_com_docker_compose_service!=""}[5m]) * 100 > 80
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High CPU Usage in Docker Container"
          description: "Container {{ .container_label_com_docker_compose_service }} is using more than 80% CPU."

      # 🚨 Alert: Frequent Container Restarts
      - alert: DockerContainerRestarting
        expr: increase(container_restart_count{container_label_com_docker_compose_service!=""}[10m]) > 3
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Docker Container Restarting Frequently"
          description: "Container {{ .container_label_com_docker_compose_service }} has restarted more than 3 times in 10 minutes."

      # 🚨 Alert: Stopped Docker Containers
      - alert: DockerContainerStopped
        expr: sum by (state) (engine_daemon_container_states_containers{state="stopped"}) > 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Stopped Container Alert"
          description: "One or more Docker containers are in a stopped state for more than 2 minutes."

      # 🚨 Alert: Paused Docker Containers
      - alert: DockerContainerPaused
        expr: sum by (state) (engine_daemon_container_states_containers{state="paused"}) > 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Paused Container Alert"
          description: "One or more Docker containers are in a paused state."

      # 🚨 Alert: No Running Containers
      - alert: NoRunningContainers
        expr: sum by (state) (engine_daemon_container_states_containers{state="running"}) == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "No Running Containers"
          description: "There are no running containers on the host."

  - name: website_alerts
    rules:
      - alert: StaticWebsiteDown
        expr: probe_success{instance="https://frissly.com"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: Static Website is Down
          description: The Static website at https://frissly.com is unreachable.

      - alert: AdminWebsiteDown
        expr: probe_success{instance="https://admin.frissly.com"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: Admin Website is Down
          description: The Admin website at https://admin.frissly.com is unreachable.

  - name: ssl_alerts
    rules:
      - alert: SSLCertificateExpiring
        expr: ssl_cert_expires_in < 30 * 24 * 60 * 60
        for: 1d
        labels:
          severity: warning
        annotations:
          summary: SSL certificate expiring soon ({{ $labels.instance }})
          description: "The SSL certificate expires in less than 30 days."

      - alert: SSLCertificateExpired
        expr: ssl_cert_expires_in <= 0
        for: 1d
        labels:
          severity: critical
        annotations:
          summary: SSL certificate expired ({{ $labels.instance }})
          description: "The SSL certificate has expired."

  - name: latency_alerts
    rules:
      - alert: HighApplicationLatency
        expr: histogram_quantile(0.99, sum by (instance, job) (rate(application_latency_seconds_bucket[5m]))) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High application latency ({{ $labels.instance }})
          description: "The 99th percentile latency is above 500ms."

      - alert: CriticalApplicationLatency
        expr: histogram_quantile(0.99, sum by (instance, job) (rate(application_latency_seconds_bucket[5m]))) > 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: Critical application latency ({{ $labels.instance }})
          description: "The 99th percentile latency is above 1000ms."
