# Kubernets interview questions
# What is Calico network 
-> Calico implements the Kubernetes Container Network Interface (CNI) as a plug-in and provides agents for Kubernetes to provide networking for containers and pods.

# What is CRD
-> CRD Stands for Custom resource Definition
-> CRDs allow users to create new types of resources without adding another API server.
-> When you create a new CustomResourceDefinition (CRD), the Kubernetes API Server creates a new RESTful resource path for each version you specify. 

# How to communicate the pod to another pod
Each pod in Kubernetes gets its own unique IP address within the cluster, even if itâ€™s running on the same node as another pod. This allows pods to communicate directly with each other using these IP addresses, without any need for port forwarding or special routing.

-> Pod-to-Pod Communication within the Same Node
If a pod in Node A needs to communicate with another pod (on the same node), it can directly use the IP of the other pod. The Kubernetes network model ensures that pods on the same node can communicate directly via their IP addresses.

   When pods are scheduled on the same node, they can communicate with each other using localhost or the loopback interface (127.0.0.1). 
   Pods on the same node can communicate directly via IP addresses or DNS names.
-> Pod-to-Pod Communication across Nodes
Each pod in Kubernetes is assigned a unique IP address. This IP is unique within the cluster, regardless of whether the pod is on the same node as another pod or on a different node.
This unique IP addressing allows pods to communicate with each other using their IPs, even across nodes.

Every pod in Kubernetes is assigned an IP address (unique within the cluster). The networking model ensures that all pods can reach each other using their respective IP addresses.
When a pod on Node-A wants to communicate with a pod on Node-B, the traffic is routed through the Kubernetes network (using the CNI plugin). The CNI plugin ensures that traffic destined for pods on different nodes reaches the correct destination, often using overlay networks or routing rules.


   When pods are scheduled on different nodes, they communicate over the cluster network. 
   Kubernetes sets up networking between nodes using a container network interface (CNI) plugin, such as Calico, Flannel, or Cilium.

# How to describe the kubernetes security
-> Enable Role-Based-Access-Contro;
-> Use Third party authentication for API server
-> Protect ETCD with TLS and Firewall
-> Isolate the kubernetes nodes
-> Monitor network traffic to limit communications
-> Use process whitelsting
-> Turn on Audit logging
-> Keep kubernets version up and Date
-> Lock Down Kubelet
-> Secure kubernetes with Aqua




# Persistence volumes data
# What is volumes in Kubernets
Kubernets will store the data in directory that should be accessiable across container in pod

# Type of kubernetes volume types
-> Local nodes types -> emprtdir, hostpath, local node type
-> Special types -> persistence volume, persistence volume claim 
-> File sharing type -> nfs
  we have lot of volume types presently we are using above types

->  emptydir -> It is used to store the data temporialy in the container. Data will available upto you pod would be existing

# First You will create the file using emptydir

apiVersion: v1
kind: Pod
metadata:
  name: my-pod1
  labels:
    app: my-nginx
spec:
  containers:
  - name: my-container
    image: manoj3003/nginx
    volumeMounts:
        - mountPath: /test
          name: test
  restartPolicy: Always
  volumes:
    - name: test
      # Define the volume type and its configuration
      emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: my-app-service
spec:
  selector:
    app: my-nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: NodePort

-> After execute the above file you will check volume mount is created or not in our local terminal
-> volume wont be create in local termianl because we are using emptydir 
-> you will enter into pod and check the volume is created or not
-> kubectl exec -it <podname> -c <containername> -- /bin/sh
-> When we use the emptydir volume if pod goes down we wont get volume.


apiVersion: v1
kind: Pod
metadata:
  name: my-pod2
  labels:
    app: my-nginx
spec:
  containers:
  - name: my-container
    image: manoj3003/nginx
    volumeMounts:
        - mountPath: /tmp/raja1
          name: raja
  - name: my-container1
    image: ashokit/javawebapp
    volumeMounts:
        - mountPath: /tmp/raja2
          name: raja
  restartPolicy: Always
  volumes:
    - name: raja
      # Define the volume type and its configuration
      emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: my-app-service1
spec:
  selector:
    app: my-nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: NodePort
 
-> After execute the above file you will check volume mount is created or not in our local terminal
-> volume wont be create in local termianl because we are using emptydir 
-> you will enter into pod and check the volume is created or not
-> kubectl exec -it <podname> -- /bin/sh
-> kubectl exec -it <podname> -c <containername> -- /bin/sh
-> When we use the emptydir volume if pod goes down we wont get volume.
-> But When add any files in container1 that files it will automatically add to container2




# How to assign singel volume to specific container

apiVersion: v1
kind: Pod
metadata:
  name: my-pod2
  labels:
    app: my-nginx
spec:
  containers:
  - name: my-container
    image: manoj3003/nginx
    volumeMounts:
        - mountPath: "/var/raja"    # location in container
          name: raja
  volumes:
    - name: raja
      hostPath:
        path: /mnt/raja           # location path
---
apiVersion: v1
kind: Service
metadata:
  name: my-app-service1
spec:
  selector:
    app: my-nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: NodePort

-> First you will execute the above file master node
-> After execute the above file. you will go workernode whereever your pods is running
-> After enter the workernode. you will follow the below commands
-> kubectl get nodes
-> After Execute the above command you will enter into the pod using below command
-> kubectl exec -it <podname> -- /bin/sh
-> After enter into the pod you will check the wheather volume is created or not
-> After check the volume you will create one file on that volume
-> After create the file you will exit from the container and check hostpath is created in your local terminal or not
-> Then you will find your hostpath and file whichever file you have created in your pod
-> Suppose if your pod goes die your data would be save in host path volumes


# How to assign singel volume to multiple container

apiVersion: v1
kind: Pod
metadata:
  name: my-pod2
  labels:
    app: my-nginx
spec:
  containers:
  - name: my-container1
    image: manoj3003/nginx
    volumeMounts:
        - mountPath: "/var/teja"    # location in container
          name: sai
  - name: my-container2
    image: ashokit/javawebapp
    volumeMounts:
        - mountPath: "/var/mani"    # location in container
          name: sai
  volumes:
    - name: sai
      hostPath:
        path: /opt/sai           # location path
---
apiVersion: v1
kind: Service
metadata:
  name: my-app-service
spec:
  selector:
    app: my-nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: NodePort  


-> First you will execute the above file master node
-> After execute the above file. you will go workernode whereever your pods is running
-> After enter the workernode. you will follow the below commands
-> kubectl get nodes
-> After Execute the above command you will enter into the container1 using below command
-> kubectl exec -it <podname> -c <containername> -- /bin/sh
-> After enter into the container1 you will check the wheather volume is created or not
-> After check the volume you will create one file on that volume
-> After create the file you will enter into second container you will check the wheather volume is created or not as well as file would being there whichever file you have created in con1
-> Later on you will create one file in con2. After create the file in container you will exit from container
-> You will check the host path and check the files whichever you have created in both container 
-> Suppose if your pod goes die your data would be save in host path volumes

Note: If you create file in one container then file will automatically add in second container as well as it will add in hostpath volume.



# How to assign a dedicated volumes to each container in a pod

apiVersion: v1
kind: Pod
metadata:
  name: my-pod2
  labels:
    app: my-nginx
spec:
  containers:
  - name: my-container1
    image: manoj3003/nginx
    volumeMounts:
        - mountPath: "/var/nagu"    # location in container
          name: nagu
  - name: my-container2
    image: ashokit/javawebapp
    volumeMounts:
        - mountPath: "/var/kittu"    # location in container
          name: kittu
  volumes:
    - name: nagu
      hostPath:
        path: /tmp/nagu            # location path
    - name: kittu
      hostPath:
        path: /tmp/kittu           # location path
---
apiVersion: v1
kind: Service
metadata:
  name: my-app-service
spec:
  selector:
    app: my-nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: NodePort
 

Note: Each container having separete local path

-> First you will execute the above file master node
-> After execute the above file. you will go workernode whereever your pods is running
-> After enter the workernode. you will follow the below commands
-> kubectl get nodes
-> After Execute the above command you will enter into the container1 using below command
-> kubectl exec -it <podname> -c <containername> -- /bin/bash
-> After enter into the container1 you will check the wheather volume is created or not
-> After check the volume you will create one file on that volume
-> After create the file you will enter into second container you will check the wheather volume is created or not as well as file would being there whichever file you have created in con1
-> Later on you will create one file in con2. After create the file in container you will exit from container
-> You will check the host path and check the files whichever you have created in both container 
-> Suppose if your pod goes die your data would be save in host path volumes

Note: If you create file in one container then file will automatically add in second container as well as it will add in hostpath volume.

Note: When you using emptydir and Hostpath in volumes. you can't share the volume to another worknode because volume create locally in worknode

-> If we want to share volume pods running on differnet nodes then we wil use NFS and persistence vloume 
-> But NFS is not working in my system. If you want to understand how it is working you follow the below youtube link

# How to assign a shared volume across all pods running on different nodes
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 2
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: javawebapp
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: my-container1
        image: ashokit/javawebapp
        ports:
        - containerPort: 8080
        volumeMounts:
        - mountPath: "/var/sup"    # location in container
          name: shanu
      - name: my-container2
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: "/var/riya"    # location in container
          name: shanu
      volumes:
       - name: shanu
         hostPath:
          path: /tmp/shanu        # location path

---
apiVersion: v1
kind: Service
metadata:
  name: my-app-service
spec:
  selector:
    app: javawebapp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: NodePort

-> After execute the above file you will follow the below link
https://youtu.be/AXi2oENUJHo?si=9VXRKkuL0aguov-j




# What is presistence volumes
-> A PersistentVolume is a piece of storage in the Kubernetes cluster that has been provisioned by an administrator.

# What is presistence volumes claim 
-> A PersistentVolumeClaim is a request for storage by a user or developer.
-> It is a resource object in the Kubernetes API that allows a Pod to access a piece of storage from a PersistentVolume (PV).

-> In persistence volume claim we have different access modes
 1) RWO -> Read write once for single node
 2) ROX -> Read only access for multiple nodes
 3) RWX -> Read write access for mulitple nodes

-> In persistence volume claim we have different policies
 1) Retain -> If we delete the pod volume will not delete
 2) Delete -> If we delete the pod volume will delete
 3) Recycle -> Recycle policy is deprecated no longer recommanded for use




# How to work presistence volumes and presistence volumes claim

# How to create a Persistent Volume (PV) 
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: my-storage-class
  hostPath:
    path: /mnt/my-pv      # mention the dir name in terminal

-> You have to execute above presistence volumes file to get storage for k8s cluster
-> kubectl get pv  --> To see the presistence volumes

# How to create a Persistent Volume claim (PVc) 
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  storageClassName: my-storage-class

-> You have to execute above presistence volumes claims file to get storage from presistence volumes
-> kubectl get pvc  --> To see the presistence volumes claims
-> kubectl get pv  --> To see the presistence volumes

# How to configue Persistent Volume claim in deployment
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: javawebapp
  template:
    metadata:
      labels:
        app: javawebapp
    spec:
      containers:
        - name: my-container
          image: ashokit/javawebapp
          volumeMounts:
            - name: my-pvc-volume
              mountPath: /path/in/container
        - name: my-container1
          image: nginx:latest
      volumes:
        - name: my-pvc-volume
          persistentVolumeClaim:
            claimName: my-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: my-app-service
spec:
  selector:
    app: javawebapp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: NodePort

-> you will execute above deployment file
-> kubectl get pods
-> kubectl get pods -o wide
-> After execute the all files. you enter into your cantainer in which worknode your pod is running
-> After enter into the container you will check the volume and create one file in that volume
-> After create the file in that volume. you will delete the pod.
-> After you delete the pod another pod will create because you mention the replica in file
-> If pod will create in another workernode. you go to that worknode and enterinto container and check the file is there whichever file you have created with previous container in that worknode 
-> If you understand more about persistence vloumes you will watch below link
https://youtu.be/hAhoeg3RryY?si=Dwtz4jDpFAYzJYyO

-> suppose if you create two pods when add the files in one pod in con then automatically files will add in another pod.
-> kubectl edit pvc <pvc-name>  --> To change persistentVolumeReclaimPolicy

Finally: when we use persistence volume will we do add volume in one worknode then it will automatically to another worknode
         when we use host volume you get file whichever workernode our pod is running. we cant get from another worknode




# Jenkins
# Jenkins interview questions

# Explain the CI/CD pipeline implemention method
-> we used github as a source code and our taget is kuberntes in our CI/CD pipeline
-> we use jenkins orchestration for CI/CD. The reason we use jenkins is whenever user make commit changes in source code webhooks trigger the declarative pipeline then using declarative pipeline we run mulitple stages
-> Stage-1 [Checkout -> Clone the code from repository]
-> Stage-2 [Build -> Build the code using maven tool]
-> Stage-3 [code scaner -> Test the code using sonar-scanner]
-> Stage-4 [build the image -> Create the docker image]
-> Stage-5 [Push the image into dockerhub]
-> Stage-6 [Deploy the code into kuberntes]

# Why we are using declarative pipeline script
-> Scripted pipeline does not have more flexibility compare to declarative pipeline.
-> Using Declarative pipeline we can easily collaborate with people and everyone can easily understand

# How do you handle the issues if your worknode is down and not responce
-> I will log into the worker node and try to understand the problems.
-> I will try lookinto the worknode the health. In case CPU and RAM going out of limit or CPU and RAM full you will write python or shell script to send alert then you will take the immediate action and implement the autoscaling to that ec2 instances.

# There is developer who has commit the change in git repo how deos your jenkins notify ?
-> Using webhooks

# What type of agnet are you using in jenkins
-> Docker agent because it is light weight and dont need do lots of installization like maven and other tools which we require


# Explain the CI/CD pipeline implemention method
-> we used github as a source code and our taget is kuberntes in our CI/CD pipeline
-> we use jenkins orchestration for CI/CD. The reason we use jenkins is whenever user make commit changes in source code webhooks trigger the declarative pipeline then using declarative pipeline we run mulitple stages

Updated Stages in the Pipeline
Stage 1: Checkout
Action: Clone the code from the GitHub repository.
Details: Jenkins checks out the latest code, including the Dockerfile and Argo CD manifests.
Stage 2: Build
Action: Build the application code.
Details: Utilize Maven (or another build tool) to compile the code and execute unit tests for quality assurance.
Stage 3: Code Scanning
Action: Perform static code analysis using SonarQube.
Details: Identify bugs, code smells, and vulnerabilities to provide feedback for developers.
Stage 4: Dependency Check
Action: Run OWASP Dependency-Check.
Details: Analyze project dependencies for known vulnerabilities. This can be integrated as a Maven plugin or run as a standalone tool. The results will be stored for review.
Stage 5: Build the Docker Image
Action: Create a Docker image.
Details: Use the Dockerfile to build the image that packages the application and its dependencies.
Stage 6: Scan Docker Image with Trivy
Action: Scan the Docker image for vulnerabilities using Trivy.
Details: After building the Docker image, use Trivy to scan it for known vulnerabilities. Store the scan results and fail the pipeline if critical vulnerabilities are found.
Stage 7: Push the Docker Image
Action: Push the Docker image to a Docker registry.
Details: Tag and upload the image to a registry (e.g., Docker Hub) for deployment.
Stage 8: Update Argo CD
Action: Update the Argo CD application configuration.
Details: Trigger Argo CD to apply updated manifests, including updating the image tag in the Kubernetes deployment.
Stage 9: Deploy to Kubernetes
Action: Argo CD handles deployment.
Details: Automatically deploys the application by syncing the Kubernetes cluster with the desired state defined in the manifests.
Stage 10: Monitoring with Dynatrace
Action: Integrate Dynatrace for monitoring and observability.
Details: After deployment, Dynatrace tracks application performance and logs for insights into health and user experience.



#File.12
# How to install java on amazon linux
sudo yum update
sudo wget https://download.java.net/java/GA/jdk13.0.1/cec27d702aa74d5a8630c65ae61e4305/9/GPL/openjdk-13.0.1_linux-x64_bin.tar.gz
sudo tar -xvf openjdk-13.0.1_linux-x64_bin.tar.gz
sudo mv jdk-13.0.1 /opt/
cd ~
cd /etc/profile.d/       	
ll
sudo vim java.sh
	JAVA_HOME='/opt/jdk-13.0.1'
	PATH="$JAVA_HOME/bin:$PATH"
	export PATH
sh java.sh
cd ~
source /etc/profile
java --version

     (or)

sudo yum update
sudo amazon-linux-extras install java-openjdk11 -y
sudo dnf install java-11-amazon-corretto -y
java --version
which java
whereis java
yum list installed | grep java
ls -al /usr/bin/java
ls -al /etc/alternatives/java  --> To see the java path

# How to install maven on amazon linux
First you need install java before you install maven (you will above java steps to install java)
sudo yum update
sudo wget https://mirrors.estointernet.in/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz
sudo tar -xvf apache-maven-3.6.3-bin.tar.gz
sudo mv apache-maven-3.6.3 /opt/
cd /etc/profile.d/                                [we have six goals in maven
ll                                                   1) mvn clean
sudo vim maven.sh                                    2) mvn compile
 	M2_HOME='/opt/apache-maven-3.6.3'            3) mvn test
	PATH="$M2_HOME/bin:$PATH"                    4) mvn package
	export PATH                                  5) mvn install
sh maven.sh                                          6) mvn deploy
cd ~
source /etc/profile
mvn --version

# How to insatll jenkins on amazon linux
First you need install java before you install jenkins (you will above java steps to install java)
sudo yum update -y
cd /opt/
sudo wget -O /etc/yum.repos.d/jenkins.repo \
    https://pkg.jenkins.io/redhat-stable/jenkins.repo
sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
sudo yum upgrade
sudo amazon-linux-extras install java-openjdk11 -y
sudo dnf install java-11-amazon-corretto -y                (You have to download maven in jenkins in configure)
sudo yum install jenkins -y                                (you will use this directory to see wheather is build or not
sudo systemctl enable jenkins                                 /var/lib/jenkins/workspace )
sudo systemctl start jenkins                         (To change the jenkins port number sudo vi /etc/sysconfig/jenkins
sudo systemctl status jenkins                          after change you do restart the jenkins) 
cat /var/lib/jenkins/secrets/initialAdminPassword   (If you forget the password to recover vi /var/lib/jenkins/config.xml
                                                                after change you do restart the jenkins) 
# How to install Tomcat
First you need install java before you install tomact (you will above java steps to install java)
you will get link by apache tomcat
sudo wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.82/bin/apache-tomcat-9.0.82.tar.gz
sudo tar -xvf apache-tomcat-9.0.82.tar.gz
cd apache-tomcat-9.0.80
cd /bin
sudo chmod 775 /bin
sudo sh startup.sh
sudo vi webapps/manager/META_INFA/contex.xml      (If you want to deploy the code into tomact server you have to installed Deploy continer plugin in jenkins)
	allow ".*"                                (cd /conf/server.xml to see port number)
cd /conf                                            
sudo vi tomcat-user.xml
  <role rolename="manager-gui"/>
  <role rolename="admin-gui"/>
  <role rolename="manager-script"/>
  <user username="tomcat" password="tomact" roles="manager-gui"/>
  <user username="admin" password="admin" roles="manager-gui,admin-gui,manager-script"/>


# How to install Sonar Qube [you need 4gb RAM t2.medium)
First you need install java before you install sonarqube (you will above java steps to install java)
sudo wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.9.2.46101.zip
sudo unzip sonarqube-8.9.2.46101.zip
sudo mv sonarqube-8.9.2.46101 /opt/
Note:- Sonar Qube server will not run with root user so that you have to create another user
sudo useradd sonar
sudo passwd sonar
sudo vim /etc/sudoers (or) sudo visudo
  sonar   ALL=(ALL)       ALL
cd /opt
sudo chown -R sonar:sonar sonarqube-8.9.2.46101
su sonar
cd /sonarqube-8.9.2.46101/bin/linux-x86-64            (cd /conf/sonar.properties to sonarqube port number)
sh sonar.sh start

if you want to deploy the code into sonar-qube by using maven
you have to do some configurion in pom.xml file
you have to write below properties to remove that properties
    <sonar.host.url>http://13.49.68.245:9000/</sonar.host.url>  [change that URL with your URL
    <sonar.login>admin</sonar.login>                            [change that login with your login id
    <sonar.password>admin</sonar.password>                      [change that password with your password

   <sonar.host.url>http://13.49.68.245:9000/</sonar.host.url>           [change that URL with your URL
   <sonar.login>8b206219f3ea7d59555b48747c1b4eba54cd7c9b</sonar.login>  [change that token with your token

if you want to deploy the code into sonar-qube by using jenkins
you need to install sonarqube-scanner plugin
you have to create credentials to sonaqube in creadentials in jenkins
you have to configure sonarqube-server in jenkins in system configure
you have to install sonar-scanner in tools in jenkins

# How to get Email Notification in jenkins
First you have to install Email-extention plugin in jenkins
go to system configure and configure Email-extention and Email
To configure both email go to youtube like vides and see.
you have to select Editable Email notification in post build actions and give email id project recipient list and apply
we may get script due to click on pipelinesynx

https://youtu.be/jIh66SjCUp4?si=ZfGQ_GwPjuBzAHeF
https://youtu.be/uMue7lNk3ew?si=nzaJE3635h5lfvMB 

# How to set the Dev,Sit and Uat Environments
First you have to create 3 instance
You have to download java and tomcat in both servers by using above java and tomact commands
You need to install build pipeline plugin in jenkins



#How to install Docker
1.sudo yum update -y
2.sudo yum install docker -y
3.sudo service docker start
4.sudo chkconfig docker on
5.sudo usermod -aG docker $USER
6.docker --version
7.docker info
If it is not work you will exit from instance and again you will login




#How to install Kubernetes
Take 3 Ubuntu Instances (20.00)
 1-Master Node (t2.medium instance)
 2-Work Nodes  ((t2.micro instance)

Install below step-1 and step-2 commands in 3 instances
Step-1:- Installing Docker
    1  sudo apt-get update
    2  sudo apt-get install docker.io
    3  docker --version
    4  sudo usermod -aG docker $USER
    5  sudo systemctl start docker
    6  sudo systemctl enable docker
    7  sudo systemctl status docker

Step-2:- Installing Kubernetes
   1  sudo apt install curl
   2  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add  --> Add GPG Kubernets key
   3  sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"    --> Add the repository
   4  sudo apt-get install kubeadm kubelet kubectl kubernetes-cni
   5  sudo apt-mark hold kubeadm kubelet kubectl kubernetes-cni
   6  kubeadm version
   7  sudo swapoff -a       --> disabel the swap
   8  sudo systemctl daemon-reload
   9  sudo systemctl start kubelet
  10  sudo systemctl enable kubelet.service
  11  sudo systemctl status kubelet.service

Below Step-3 commands execute in only master node
Step-3:- Running and Deploying Kubernetes
    1  sudo kubeadm init
    2  mkdir -p $HOME/.kube
    3  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    4  sudo chown $(id -u):$(id -g) $HOME/.kube/config
    5  kubectl get nodes
    6  kubeadm token create --print-join-command
Copy that token and past that token in worknodes using sudo  --> worknodes connect to masternode
    7  kubectl get nodes

Create the one ubuntu instance to install kubectl (20.00)
Step-4:- Install kubectl in above instance 
1 sudo apt-get update
2 curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
3 chmod +x kubectl
4 sudo mv kubectl /usr/local/bin/
5 kubectl version --client

How to connect Kubectl to the master node
1 mkdir ~/.kube         --> execute in kubectl
 Note: Take kube config file from master and it keep it here
2 cat ~/.kube/config    --> executive in master node and copy the that data
3 vi ~/.kube/config     --> past here that copy data
4 kubectl cluster-info  --> execute this command in kubectl  --> it showing wheather kubectl is connected to the master node or not
5 kubectl get nodes     --> execute in both kubectl and master to get pods are connected or not
6 sudo systemctl restart kubelet.service   --> if pods are not showing then you will execute this command in master

When you get status is Notready then we will below command
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

We have to follow below steps in worknodes also to get pods even in work nodes
1 mkdir ~/.kube         --> execute in kubectl
 Note: Take kube config file from master and it keep it here
2 cat ~/.kube/config    --> executive in master node and copy the that data
3 vi ~/.kube/config     --> past here that copy data
4 kubectl cluster-info  --> execute this command in kubectl  --> it showing wheather kubectl is connected to the master node or not
5 kubectl get nodes     --> execute in both kubectl and master to get pods are connected or not

#If you want to do auto-scalling you need to install metric server in master node
sudo yum install git -y                 
git --version
git clone https://github.com/ashokitschool/k8s_metrics_server
cd k8s_metrics_server/
cd deploy/
cd 1.8+/
kubectl apply -f .
kubectl get namespaces
kubectl get pods -n kube-system
kubectl top nodes  --> To get CPU memory

Do you want to check wheather autoscalling working or not
To increase the load on pods you will use below commands in duplicate
kubectl run -it loadgenerator --image=busybox
wget -q -O- http:my-app-service










#How install terraform
sudo yum install -y yum-utils
sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
sudo yum -y install terraform
terraform --version
terraform init
terraform fmt
terraform validate
terraform plan
terraform apply
terraform apply -auto-apporve
terraform destroy

#How to install Grafana
1  sudo yum update -y
2  sudo yum install -y fontconfig freetype-devel urw-fonts
3  sudo vi /etc/yum.repos.d/grafana.repo
[grafana]
name=grafana
baseurl=https://packages.grafana.com/oss/rpm
repo_gpgcheck=1
enabled=1
gpgcheck=1
gpgkey=https://packages.grafana.com/gpg.key
sslverify=1

4  sudo yum install -y grafana
5  sudo systemctl start grafana-server
6  sudo systemctl enable grafana-server
7  sudo systemctl status grafana-server
8  sudo systemctl is-enabled grafana-server
9  sudo systemctl enable grafana-server

#How to install influxdb
1) wget https://dl.influxdata.com/influxdb/releases/influxdb2-2.7.1.x86_64.rpm
2) sudo yum localinstall influxdb2-2.7.1.x86_64.rpm
3) sudo service influxdb start
4) sudo service influxdb status
5) cd /etc/default/influxdb2
6) cd /etc/default/
7) ll
8) sudo vi influxd
	ARG1="--http-bind-address :8087"
9) Edit the /lib/systemd/system/influxdb.service file as follows:
	ExecStart=/usr/bin/influxd $ARG1
11) wget https://dl.influxdata.com/influxdb/releases/influxdb2-client-2.3.0-linux-amd64.tar.gz
12) ll
13) tar xvzf influxdb2-client-2.3.0-linux-amd64.tar.gz
14) ll
15) cd influxdb2-client-2.3.0-linux-amd64
16) ll
17) pwd
18) cd ~
19) sudo cp /home/ec2-user/influxdb2-client-2.3.0-linux-amd64/influx /usr/local/bin/
20) influx version

#How to install AWSCLI
sudo yum update -y
sudo yum install python3-pip -y
sudo pip3 install awscli
aws --version
aws configure
aws sts get-caller-identity

#How to install KUBECTL
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl
sudo mv kubectl /usr/local/bin/
kubectl version --client


#Kubernet connect to kubectl 
aws eks --region ap-south-1 describe-cluster --name eks-1 --query cluster.status
aws eks --region ap-south-1 update-kubeconfig --name eks-1
kubectl get svc
	
#RDS details
sudo -i
yum update -y
yum install mariadb-server
systemctl enable mariadb
systemctl start mariadb

username: admin
password: Varma_3003
RDS Endpoint: demo-rds-database.cnxpob7pwepk.ap-south-1.rds.amazonaws.com

Command to connect 
mysql -h <paste-rds-endpoint-here> -P 3306 -u rduser -p

mysql -h demo-rds-database.cnxpob7pwepk.ap-south-1.rds.amazonaws.com -P 3306 -u rduser -p


# How to install Cloud Wacth agent commands
    1  sudo yum update -y
    2  sudo yum install httpd -y
    3  sudo service httpd start
    4  sudo service httpd status
    5  cd /var/www/html/
    6  ll
    7  sudo vi index.html
    8  /var/log/httpd/access_log
    9  /var/log/httpd/error_log
    8  sudo yum install amazon-cloudwatch-agent -y
    9  sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c ssm:AmazonCloudWatch-linux
   10  sudo mkdir /usr/share/collectd/
   11  sudo touch /usr/share/collectd/types.db
   12  ll /usr/share/collectd/types.db
   13  sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c ssm:AmazonCloudWatch-linux
   14  sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a status

#How to install artifactory packages
--> Get the link from https://jfrog.com/community/open-source/
sudo wget https://releases.jfrog.io/artifactory/bintray-artifactory/org/artifactory/oss/jfrog-artifactory-oss/7.68.7/jfrog-artifactory-oss-7.68.7-linux.tar.gz
sudo tar -xvf jfrog-artifactory-oss-7.68.7-linux.tar.gz
cd artifactory-oss-7.68.7
cd app
cd bin
sudo sh artifactory.sh



#How to install nagios
sudo yum install httpd php gcc glibc glibc-common gd gd-devel make net-snmp
   74  sudo useradd nagios
   75  sudo groupadd nagcmd
   76  sudo usermod -a -G nagcmd nagios
   77  cd /tmp
   78  wget https://assets.nagios.com/downloads/nagioscore/releases/nagios-4.4.6.tar.gz
   79  tar -zxvf nagios-4.4.6.tar.gz
   80  ll
   81  cd nagios-4.4.6/
   82  ll
   83  ./configure --with-command-group=nagcmd
   84  make all
   85  sudo make install
   86  sudo make install-init
   87  sudo make install-daemoninit
   88  sudo make install-config
   89  sudo make install-commandmode
   90  sudo make install-exfoliation
   91  cd ..
   92  wget https://nagios-plugins.org/download/nagios-plugins-2.3.3.tar.gz
   93  tar -zxvf nagios-plugins-2.3.3.tar.gz
   94  cd nagios-plugins-2.3.3
   95  ./configure --with-nagios-user=nagios --with-nagios-group=nagios
   96  make
   97  sudo make install
   98  sudo vi /etc/httpd/conf.d/nagios.conf
   99  sudo htpasswd -c /usr/local/nagios/etc/htpasswd.users your_username
  100  sudo systemctl start httpd
  101  sudo systemctl start nagios
  102  sudo systemctl enable httpd
  103  sudo systemctl enable nagios
  104  cat /etc/passwd
  105  cd /usr/local/
  106  cd nagios/
  107  cd etc/
  108  ll
  109  vi htpasswd.users



# To use the docker private image for kubernets deployment. we have to create secret using below command
kubectl create secret docker-registry regcred --docker-server=https://index.docker.io/v1/ --docker-username=manoj3003 --docker-password=Varma_3003 --namespace=webapps
